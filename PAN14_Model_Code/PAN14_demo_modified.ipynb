{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt4R6YMSF7sG"
      },
      "source": [
        "# Set Up\n",
        "Set up the Google Colab environment and import dependent libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rlYEd4wF2tU",
        "outputId": "93c35f8f-bea0-4d97-8f4d-36f03417e178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have a sneaking suspicion that I'm not running on Google Colab\n"
          ]
        }
      ],
      "source": [
        "#Loading data from Google drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  os.chdir(\"/content/drive/My Drive/PAN14_Code\")\n",
        "  RUNNING_COLAB = True\n",
        "except ImportError:\n",
        "  print(\"I have a sneaking suspicion that I'm not running on Google Colab\")\n",
        "  RUNNING_COLAB = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TqfYit_PWqhW"
      },
      "outputs": [],
      "source": [
        "# Lambda to print module versions\n",
        "ver = lambda module : print(f\"{module.__name__}=={module.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supress Tensorflow warnings (usually unnecessary,\n",
        "#\tbut supresses some annoying warnings that happen when models are overwritten during testing)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj0ew-WfF-Fv",
        "outputId": "d250a9f1-6768-46e3-c331-a70bf30d531d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy==1.24.3\n",
            "nltk==3.8.1\n",
            "tensorflow==2.13.0\n",
            "gensim==4.3.1\n",
            "networkx==3.1\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import math\n",
        "import csv\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import itertools\n",
        "from collections import Counter\n",
        "ver(np)\n",
        "\n",
        "#pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download([\"punkt\", \"stopwords\",\"wordnet\"])\n",
        "ver(nltk)\n",
        "\n",
        "#pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "ver(tf)\n",
        "\n",
        "#pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import common_texts\n",
        "ver(gensim)\n",
        "\n",
        "#pip install scikit-learn\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "from collections import defaultdict\n",
        "\n",
        "#pip install networkx\n",
        "import networkx as nx\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlretrieve\n",
        "ver(nx)\n",
        "\n",
        "#pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGXneK8UW8EK",
        "outputId": "b1358140-6037-4b23-c8df-5e4477876886"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to ./nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "def nltk_setup(path = None):\n",
        "  \"\"\"Initialize and download the right modules\"\"\"\n",
        "  if type(path) is None:\n",
        "    # No change to default path (defaults to user home directory)\n",
        "    nltk.download([\"punkt\", \"stopwords\",\"wordnet\"])\n",
        "  else:\n",
        "    # Change default path\n",
        "    nltk.data.path = [ path ]\n",
        "    nltk.download([\"punkt\", \"stopwords\",\"wordnet\"], download_dir=nltk.data.path[0])\n",
        "\n",
        "if RUNNING_COLAB:\n",
        "  nltk_setup()\n",
        "else:\n",
        "  nltk_setup(f\"{os.path.curdir}/nltk_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82RifHrL0W8S",
        "outputId": "2c21ec79-e0ff-4b9c-cdab-a97c12dd21c8"
      },
      "outputs": [],
      "source": [
        "def print_colab_stats():\n",
        "  # GPU info\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "\n",
        "  # Memory Info\n",
        "  from psutil import virtual_memory\n",
        "  ram_gb = virtual_memory().total / 1e9\n",
        "  print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "  if ram_gb < 20:\n",
        "    print('Not using a high-RAM runtime')\n",
        "  else:\n",
        "    print('You are using a high-RAM runtime!')\n",
        "\n",
        "if RUNNING_COLAB: print_colab_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mepv72EIF_kq"
      },
      "source": [
        "# Data Process\n",
        "Read the data in and save it in the dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PiwPGxTzI5Gl"
      },
      "outputs": [],
      "source": [
        "def get_data_directory_path(subdirectory):\n",
        "    return os.path.join('data', subdirectory)\n",
        "\n",
        "\n",
        "def get_json_file_path(data_directory, file_name):\n",
        "    return os.path.join(data_directory, file_name)\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_text_from_files(file_paths):\n",
        "    known_text, unknown_text = [], []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        text_lines = []\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                cleaned_line = line.strip().lstrip(\"\\ufeff\")\n",
        "                text_lines.append(cleaned_line)\n",
        "        if 'unknown' in file_path:\n",
        "            unknown_text.append(text_lines)\n",
        "        else:\n",
        "            known_text.append(text_lines)\n",
        "\n",
        "    return known_text, unknown_text\n",
        "\n",
        "\n",
        "def build_corpus(data_directory, content_data, label_data):\n",
        "    corpus = {}\n",
        "\n",
        "    for index in tqdm(range(len(content_data['problems']))):\n",
        "        problem_file_paths = glob.glob(os.path.join(data_directory, content_data['problems'][index], '*'))\n",
        "\n",
        "        if not problem_file_paths:\n",
        "            continue\n",
        "\n",
        "        known_text, unknown_text = extract_text_from_files(problem_file_paths)\n",
        "        label = 1 if label_data['problems'][index]['answer'] == 'Y' else 0\n",
        "\n",
        "        corpus[index] = {\n",
        "            'known': known_text,\n",
        "            'unknown': unknown_text,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OVzAw15TL8CA"
      },
      "outputs": [],
      "source": [
        "# Get data path\n",
        "train_data_directory = get_data_directory_path('train_data')\n",
        "validation_data_directory = get_data_directory_path('val_data')\n",
        "test_data_directory = get_data_directory_path('test_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SebVk4WpI9c2"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "train_content = read_json_file(get_json_file_path(train_data_directory, 'contents.json'))\n",
        "train_labels = read_json_file(get_json_file_path(train_data_directory, 'truth.json'))\n",
        "# # Val\n",
        "#validation_content = read_json_file(get_json_file_path(validation_data_directory, 'contents.json'))\n",
        "#validation_labels = read_json_file(get_json_file_path(validation_data_directory, 'truth.json'))\n",
        "# Test\n",
        "test_content = read_json_file(get_json_file_path(test_data_directory, 'contents.json'))\n",
        "test_labels = read_json_file(get_json_file_path(test_data_directory, 'truth.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhTF9bRPL7Ac",
        "outputId": "47bd2a53-40e1-4994-ff45-d2300415b79b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 3560.76it/s]\n",
            "100%|██████████| 200/200 [00:00<00:00, 3563.43it/s]\n"
          ]
        }
      ],
      "source": [
        "# Get train corpus\n",
        "train_corpus = build_corpus(train_data_directory, train_content, train_labels)\n",
        "# # Get val corpus\n",
        "#val_corpus = build_corpus(validation_data_directory, validation_content, validation_labels)\n",
        "# Get test corpus\n",
        "test_corpus = build_corpus(test_data_directory, test_content, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the training data into training and validation\n",
        "def split_data_into_train_and_val(data_dict, test_size=0.2, random_state=42):\n",
        "    document_ids, labels = zip(*[(doc_id, data['label']) for doc_id, data in data_dict.items()])\n",
        "\n",
        "    train_ids, val_ids, train_labels, val_labels = train_test_split(document_ids, labels, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    train_data = {doc_id: data_dict[doc_id] for doc_id in train_ids}\n",
        "    validation_data = {doc_id: data_dict[doc_id] for doc_id in val_ids}\n",
        "\n",
        "    return train_data, validation_data\n",
        "\n",
        "train_corpus, val_corpus = split_data_into_train_and_val(data_dict=train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkBeR4eOPEm2"
      },
      "source": [
        "# Train Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wuFlKWaRJ1c"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a given text by tokenizing, removing punctuation and numbers,\n",
        "    removing stop words, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        list: The preprocessed text as a list of tokens.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    table = str.maketrans('', '', string.punctuation + string.digits)\n",
        "    tokens = [word.translate(table) for word in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if (not word in stop_words) and (word != '')]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def train_word2vec_model(data, vector_size):\n",
        "    \"\"\"\n",
        "    Train a word2vec model using the given data.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The data to use for training the model.\n",
        "        vector_size (int): The size of the word vectors in the model.\n",
        "\n",
        "    Returns:\n",
        "        gensim.models.Word2Vec: The trained word2vec model.\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "\n",
        "    # Process all articles in the data\n",
        "    for articles in tqdm(data.values(), total=len(data)):\n",
        "        all_articles = []\n",
        "        all_articles.extend(articles['known'])\n",
        "        all_articles.extend(articles['unknown'])\n",
        "\n",
        "        for article in all_articles:\n",
        "            for line in article:\n",
        "                text = line.strip()\n",
        "                tokens = preprocess_text(text)\n",
        "                corpus.append(tokens)\n",
        "\n",
        "    # Train the word2vec model\n",
        "    word2vec_model = gensim.models.Word2Vec(vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "    word2vec_model.build_vocab(corpus)\n",
        "    word2vec_model.train(corpus, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
        "\n",
        "    return word2vec_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVvsP3-_RS5_"
      },
      "outputs": [],
      "source": [
        "# Size of word vectors in the word2vec model\n",
        "w2v_vector_size = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejtm_oXrRP4-",
        "outputId": "0909a260-3371-4864-8a10-e179c339b56d"
      },
      "outputs": [],
      "source": [
        "# Train a word2vec model using the training corpus\n",
        "word2vec_model = train_word2vec_model(train_corpus, w2v_vector_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFDDT_rmRokt"
      },
      "source": [
        "# Vectorize Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDsYcGE-TyUc"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_vector(texts, model):\n",
        "    \"\"\"\n",
        "    Convert a list of texts into their corresponding word2vec vectors\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for text in texts:\n",
        "        words = preprocess_text(text)\n",
        "        vector = np.sum([model.wv[word] for word in words if word in model.wv], axis=0)\n",
        "        word_count = np.sum([word in model.wv for word in words])\n",
        "        if word_count != 0:\n",
        "            vector /= word_count\n",
        "        else:\n",
        "          vector = np.zeros(w2v_vector_size)\n",
        "        vectors.append(vector)\n",
        "    return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F6DWIL9UZIw"
      },
      "outputs": [],
      "source": [
        "def count_punctuations(texts):\n",
        "  \"\"\"\n",
        "  Count the frequency of different punctuations in the texts\n",
        "  \"\"\"\n",
        "  # Define punctuations to count\n",
        "  punctuations = set(['.', ',', ';', ':', '!', '?', '-', '(', ')', '\\\"', '\\'', '`', '/'])\n",
        "\n",
        "  # Initialize dictionary to count punctuations\n",
        "  punctuations_count = {p: 0 for p in punctuations}\n",
        "\n",
        "  # Count punctuations in text_list\n",
        "  for text in texts:\n",
        "      for char in text:\n",
        "          if char in punctuations:\n",
        "              punctuations_count[char] += 1\n",
        "\n",
        "  # Return list of punctuation counts\n",
        "  return list(punctuations_count.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_IQDJiqUX7C"
      },
      "outputs": [],
      "source": [
        "def analyze_sentence_lengths(sentences):\n",
        "  \"\"\"\n",
        "  Analyze the lengths of sentences\n",
        "  \"\"\"\n",
        "  sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
        "  average_length = np.mean(sentence_lengths)\n",
        "  count_over_avg = np.sum([length > average_length for length in sentence_lengths])\n",
        "  count_under_avg = np.sum([length < average_length for length in sentence_lengths])\n",
        "  count_avg = len(sentence_lengths) - count_over_avg - count_under_avg\n",
        "\n",
        "  return [count_over_avg, count_under_avg, count_avg, average_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pevRcCjGUWyz"
      },
      "outputs": [],
      "source": [
        "def analyze_words(texts):\n",
        "    \"\"\"\n",
        "    Analyze the words used in the texts\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for text in texts:\n",
        "        tokenized = word_tokenize(text.lower())\n",
        "        processed = [lemmatizer.lemmatize(word) for word in tokenized if word not in stop_words]\n",
        "        words += processed\n",
        "    word_freq = nltk.FreqDist(words)\n",
        "    rare_count = np.sum([freq <= 2 for word, freq in word_freq.items()])\n",
        "    long_count = np.sum([len(word) > 6 for word in words])\n",
        "    word_lengths = [len(word) for word in words]\n",
        "    average_length = np.mean(word_lengths)\n",
        "    count_over_avg = np.sum([length > average_length for length in word_lengths])\n",
        "    count_under_avg = np.sum([length < average_length for length in word_lengths])\n",
        "    count_avg = len(word_lengths) - count_over_avg - count_under_avg\n",
        "    ttr = len(set(words)) / len(words) if words else 0\n",
        "\n",
        "    return [rare_count, long_count, count_over_avg, count_under_avg, count_avg, ttr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoTx1ZxWUUOE"
      },
      "outputs": [],
      "source": [
        "def calculate_style_vector(texts):\n",
        "  \"\"\"\n",
        "  Calculate the style vector of the texts\n",
        "  \"\"\"\n",
        "  punctuation_vec = count_punctuations(texts)     # Punctuations stylistic features\n",
        "  sentence_vec = analyze_sentence_lengths(texts)  # Sentences stylistic features\n",
        "  word_vec = analyze_words(texts)                 # Words stylistic features\n",
        "  word_count = np.sum([len(text.split()) for text in texts])\n",
        "\n",
        "  vector = np.concatenate((punctuation_vec, sentence_vec, word_vec))\n",
        "\n",
        "  return vector / word_count if word_count else vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsJRlqF8gHyO"
      },
      "outputs": [],
      "source": [
        "def get_vectors(texts, w2v_model):\n",
        "  res = []\n",
        "  for text in texts:\n",
        "    w2v_vec = np.mean(convert_text_to_vector(text, w2v_model), axis=0)\n",
        "    style_vec = calculate_style_vector(text)\n",
        "    res.append(np.concatenate((w2v_vec, style_vec), axis=None))\n",
        "    # res.append(w2v_vec)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEYwCl8yR0D_"
      },
      "outputs": [],
      "source": [
        "def vectorize_text_data(data, w2v_model):\n",
        "  \"\"\"\n",
        "  Build author data from the corpus\n",
        "  \"\"\"\n",
        "  res = {}\n",
        "  for key,val in tqdm(data.items(), total=len(data)):\n",
        "    if len(val['unknown']) == 0:\n",
        "      continue\n",
        "    res[key] = {\n",
        "        'known': get_vectors(val['known'], w2v_model),\n",
        "        'unknown': get_vectors(val['unknown'], w2v_model),\n",
        "        'label': val['label']\n",
        "    }\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP_a-J_xX4Vf",
        "outputId": "d332ef49-f77a-431c-a8e0-9f93a99c9b05"
      },
      "outputs": [],
      "source": [
        "train_data = vectorize_text_data(train_corpus, word2vec_model)\n",
        "val_data = vectorize_text_data(val_corpus, word2vec_model)\n",
        "test_data = vectorize_text_data(test_corpus, word2vec_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THrYBCxhimox"
      },
      "source": [
        "# Build Triplet Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5t503c3ip6H"
      },
      "outputs": [],
      "source": [
        "# Random triplet mining\n",
        "def build_random_triplet_sample(data):\n",
        "  \"\"\"\n",
        "  This function creates random triplet samples from the input data\n",
        "  \"\"\"\n",
        "\n",
        "  keys_list = list(data.keys())\n",
        "  triplet_samples = {}\n",
        "\n",
        "  # Initialize the lists for storing the anchor, positive, and negative samples\n",
        "  anchors, positives, negatives = [], [], []\n",
        "\n",
        "  for key,val in tqdm(data.items(), total=len(data)):\n",
        "    n = len(val['known'])\n",
        "    for i in range(n):\n",
        "      for j in range(i+1, n):\n",
        "        anchors.append(val['known'][i])\n",
        "        positives.append(val['known'][j])\n",
        "        # Get negative sample\n",
        "        while True:\n",
        "          random_key = random.choices(keys_list, k=1)\n",
        "          if random_key != key:\n",
        "            break\n",
        "        random_neg_sample = random.choices(data[random_key[0]]['known'], k=1)\n",
        "        negatives.append(random_neg_sample[0])\n",
        "\n",
        "\n",
        "  # Build triplet sample\n",
        "  for i in range(len(anchors)):\n",
        "    triplet_samples[i] = {\n",
        "        'anchor': anchors[i],\n",
        "        'positive': positives[i],\n",
        "        'negative': negatives[i]\n",
        "    }\n",
        "\n",
        "  return triplet_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu8oSknIi65X",
        "outputId": "a68cd853-52e8-4bb7-adcb-c3cfd8a48f80"
      },
      "outputs": [],
      "source": [
        "random_triplet_samples = build_random_triplet_sample(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8eVVLqAmen1"
      },
      "outputs": [],
      "source": [
        "anchor_data = np.array([data['anchor'] for data in random_triplet_samples.values()])\n",
        "positive_data = np.array([data['positive'] for data in random_triplet_samples.values()])\n",
        "negative_data = np.array([data['negative'] for data in random_triplet_samples.values()])\n",
        "labels_data = np.array([0 for _ in random_triplet_samples.values()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMdu9OrJk35",
        "outputId": "6e7d73a0-ab58-473b-ffa0-2aebf54698f3"
      },
      "outputs": [],
      "source": [
        "val_random_triplet_samples = build_random_triplet_sample(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1Qc49FzJl2K"
      },
      "outputs": [],
      "source": [
        "val_anchor_data = np.array([data['anchor'] for data in val_random_triplet_samples.values()])\n",
        "val_positive_data = np.array([data['positive'] for data in val_random_triplet_samples.values()])\n",
        "val_negative_data = np.array([data['negative'] for data in val_random_triplet_samples.values()])\n",
        "val_labels_data = np.array([0 for _ in val_random_triplet_samples.values()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryJTRnGkmlqm"
      },
      "source": [
        "# Build SiameseNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmK7NtP31jdn"
      },
      "source": [
        "## Model Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovKbyGoM079s"
      },
      "outputs": [],
      "source": [
        "class SiameseNet(tf.keras.Model):\n",
        "    def __init__(self, base_network, clf_network):\n",
        "        super().__init__()\n",
        "        self.base = base_network\n",
        "        self.clf = clf_network\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor = inputs[0]\n",
        "        positive = inputs[1]\n",
        "        negative = inputs[2]\n",
        "\n",
        "        output_anchor = self.base(anchor)\n",
        "        output_positive = self.base(positive)\n",
        "        output_negative = self.base(negative)\n",
        "\n",
        "        # Anchor - Positive\n",
        "        x1 = tf.concat([output_anchor, output_positive], axis=-1)\n",
        "        x1_out = self.clf(x1)\n",
        "\n",
        "        # Anchor - Negative\n",
        "        x2 = tf.concat([output_anchor, output_negative], axis=-1)\n",
        "        x2_out = self.clf(x2)\n",
        "\n",
        "        return (x1_out, x2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGlosr0W7loe"
      },
      "outputs": [],
      "source": [
        "def create_dense_block(x, units, dropout_rate, l1_reg, l2_reg):\n",
        "    x = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return tf.keras.layers.Dropout(dropout_rate)(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10yA51LpmofZ"
      },
      "outputs": [],
      "source": [
        "# Define the base network\n",
        "def create_base_network(embedding_dim, dropout_rate=0.4, l1_reg=0.001, l2_reg=0.001):\n",
        "    input = tf.keras.layers.Input(shape=embedding_dim)\n",
        "    x = tf.keras.layers.BatchNormalization()(input)\n",
        "\n",
        "    x = create_dense_block(x, 256, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 128, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 64, dropout_rate, l1_reg, l2_reg)\n",
        "\n",
        "    x = tf.keras.layers.Dense(64, activation='linear')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=input, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW83mAPuzCDW"
      },
      "outputs": [],
      "source": [
        "def create_clf_network(input_shape, dropout_rate=0.5, l1_reg=0.003, l2_reg=0.003):\n",
        "    input = tf.keras.layers.Input(shape=(input_shape,))\n",
        "    x = tf.keras.layers.BatchNormalization()(input)\n",
        "\n",
        "    x = create_dense_block(x, 128, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 64, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 32, dropout_rate, l1_reg, l2_reg)\n",
        "\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=input, outputs=x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSgJG94HkwYj"
      },
      "outputs": [],
      "source": [
        "def customer_loss(y_true, y_pred):\n",
        "    AP = y_pred[0]\n",
        "    AN = y_pred[1]\n",
        "\n",
        "    loss = 1.0 - AP + AN\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC2AmF8om5V4"
      },
      "source": [
        "## Construct the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iPD1M5im982"
      },
      "outputs": [],
      "source": [
        "# Define the embedding dimension\n",
        "embedding_dim = anchor_data[0].shape\n",
        "\n",
        "# Create base network\n",
        "base_network = create_base_network(embedding_dim)\n",
        "clf_network = create_clf_network(base_network.output_shape[1]*2)\n",
        "\n",
        "siamese_model = SiameseNet(base_network, clf_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8aeEYUGm__W"
      },
      "outputs": [],
      "source": [
        "input_anchor = tf.keras.layers.Input(shape=embedding_dim)\n",
        "input_positive = tf.keras.layers.Input(shape=embedding_dim)\n",
        "input_negative = tf.keras.layers.Input(shape=embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FijK9xqnCor"
      },
      "outputs": [],
      "source": [
        "# Assemble siameseNet model\n",
        "siamese_model.compile(optimizer='adam',\n",
        "                      loss=customer_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtusEHcYnFAZ"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"model_weights/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_save = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                             save_weights_only=True,\n",
        "                                             verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssE3i-v1nJZe"
      },
      "source": [
        "## Load SiameseNet Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oizjc50UnGYr"
      },
      "outputs": [],
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "siamese_model.load_weights(latest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo08aYawnUFS"
      },
      "source": [
        "## Train SiameseNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJAUUc9EnWpF"
      },
      "source": [
        "### Train on Random Triplet Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VgFc5gnnZZa",
        "outputId": "a0bf01f7-869e-4b2a-d1f7-537215205753"
      },
      "outputs": [],
      "source": [
        "# Train siameseNet model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
        "siamese_history = siamese_model.fit([anchor_data, positive_data, negative_data], labels_data,\n",
        "                  epochs=1000,\n",
        "                  validation_data=([val_anchor_data, val_positive_data, val_negative_data], val_labels_data),\n",
        "                  callbacks=[early_stopping, cp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kYqKKoldWObe",
        "outputId": "4f0a83b8-8ccc-44c6-f8e2-8dedf88a0179"
      },
      "outputs": [],
      "source": [
        "loss = siamese_history.history['loss']\n",
        "val_loss = siamese_history.history['val_loss']\n",
        "\n",
        "# Draw\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRB88fJtngeR"
      },
      "source": [
        "### Train on Semi-Hard Triplet Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yktUMRHfrUfQ"
      },
      "source": [
        "#### Semi-Hard Samples Construct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7n3UK1InkZ0"
      },
      "outputs": [],
      "source": [
        "# Build semi-hard triplet sample candidates\n",
        "def build_triplet_sample_candidates(data):\n",
        "  res = {}\n",
        "\n",
        "  keys = []\n",
        "  anchors = []\n",
        "  positives = []\n",
        "\n",
        "  for key,val in tqdm(data.items(), total=len(data)):\n",
        "    n = len(val['known'])\n",
        "    for i in range(n-1):\n",
        "      keys.append(key)\n",
        "      anchors.append(val['known'][i])\n",
        "      positives.append(val['known'][i+1:])\n",
        "\n",
        "  for i in range(len(keys)):\n",
        "    res[i] = {\n",
        "        'key': keys[i],\n",
        "        'anchor': anchors[i],\n",
        "        'positives': positives[i]\n",
        "    }\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3IvyvFBpmYp",
        "outputId": "d6a4b5b3-dd1e-4b3f-9fec-146837e21713"
      },
      "outputs": [],
      "source": [
        "triplet_sample_candidates = build_triplet_sample_candidates(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58rM10jr35k4"
      },
      "outputs": [],
      "source": [
        "def create_negative_vectors_dict(data):\n",
        "    negative_vectors_dict = {}\n",
        "    key_list = list(data.keys())\n",
        "\n",
        "    for key in tqdm(key_list, total=len(key_list)):\n",
        "        negative_vectors_dict[key] = []\n",
        "        for k,v in data.items():\n",
        "            if k != key:\n",
        "                for vec in v['known']:\n",
        "                    negative_vectors_dict[key].append(vec)\n",
        "    return negative_vectors_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvTKNBCw5Hgp",
        "outputId": "8f7bfa7e-8be5-4706-e27b-32ed3896e448"
      },
      "outputs": [],
      "source": [
        "negative_vectors_dict = create_negative_vectors_dict(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGz9M-KEqaAw"
      },
      "outputs": [],
      "source": [
        "def select_random_from_list(input_list):\n",
        "    \"\"\"\n",
        "    Selects a random item from a list.\n",
        "    \"\"\"\n",
        "    return input_list[np.random.randint(0, len(input_list))]\n",
        "\n",
        "# def select_negative_vectors(negatives, key):\n",
        "#     \"\"\"\n",
        "#     Collects all negative vectors except for the one corresponding to the key.\n",
        "#     \"\"\"\n",
        "#     return [vec for k,v in negatives.items() if k != key for vec in v['known']]\n",
        "\n",
        "def get_random_triplet(sample, negatives):\n",
        "    \"\"\"\n",
        "    This function takes a sample and negatives, and returns a random triplet of anchor, positive, and negative.\n",
        "    \"\"\"\n",
        "    # Select the positive vector\n",
        "    positive = select_random_from_list(sample['positives'])\n",
        "\n",
        "    # Select the negative vector\n",
        "    negative = select_random_from_list(negatives[sample['key']])\n",
        "\n",
        "    return sample['anchor'], positive, negative\n",
        "\n",
        "def get_hard_triplet(sample, negatives, base_model, clf_model,):\n",
        "    \"\"\"\n",
        "    This function takes a sample, negatives, and a model, and returns a hard triplet of anchor, positive, and negative.\n",
        "    The sample with the lowest probability is the hardest positive sample,\n",
        "    while a high probability indicates that the model is confident in classifying it as positive.\n",
        "    Therefore, the lowest probability implies that the model has incorrectly classified it.\n",
        "    \"\"\"\n",
        "    anchor_rep = base_model.predict(np.array([sample['anchor']]), verbose=0)\n",
        "\n",
        "    ### ------ Positive ------ ###\n",
        "    # Compute distances between anchor and all positive vectors\n",
        "    positive_reps = base_model.predict(np.array(sample['positives']), verbose=0)\n",
        "    AP_reps = []\n",
        "    for rep in positive_reps:\n",
        "        comb = np.concatenate((anchor_rep[0], rep), axis=None)\n",
        "        AP_reps.append(comb)\n",
        "\n",
        "    # Select the hardest positive (the one with the lowest probability)\n",
        "    positive_distances = clf_model.predict(np.array(AP_reps), verbose=0)\n",
        "    hardest_positive = sample['positives'][np.argmin(positive_distances)]\n",
        "\n",
        "\n",
        "    ### ------ Negative ------ ###\n",
        "    # Collect all negative vectors and compute distances to anchor\n",
        "    negative_vectors = negatives[sample['key']]\n",
        "    negative_reps = base_model.predict(np.array(negative_vectors), verbose=0)\n",
        "    AN_reps = []\n",
        "    for rep in negative_reps:\n",
        "        comb = np.concatenate((anchor_rep[0], rep), axis=None)\n",
        "        AN_reps.append(comb)\n",
        "\n",
        "    # Select the hardest negative (the one with the highest probability)\n",
        "    negative_distances = clf_model.predict(np.array(AN_reps), verbose=0)\n",
        "    hardest_negative = negative_vectors[np.argmax(negative_distances)]\n",
        "\n",
        "    # # positive_distances = [compute_cosine_distance(pos_rep, anchor_rep[0]) for pos_rep in positive_reps]\n",
        "    # positive_distances = [np.sum(np.square(pos_rep - anchor_rep[0])) for pos_rep in positive_reps]\n",
        "\n",
        "    # # Select the hardest positive (the one with the largest distance)\n",
        "    # hardest_positive = sample['positives'][np.argmax(positive_distances)]\n",
        "\n",
        "    # # Collect all negative vectors and compute distances to anchor\n",
        "    # negative_vectors = select_negative_vectors(negatives, sample['key'])\n",
        "    # negative_reps = model.predict(np.array(negative_vectors), verbose=0)\n",
        "    # negative_distances = [np.sum(np.square(neg_rep - anchor_rep[0])) for neg_rep in negative_reps]\n",
        "\n",
        "    # # Select the hardest negative (the one with the smallest distance)\n",
        "    # hardest_negative = negative_vectors[np.argmin(negative_distances)]\n",
        "\n",
        "    return sample['anchor'], hardest_positive, hardest_negative\n",
        "\n",
        "def get_triplet(sample, negatives, base_model, clf_model, hard_triplet_probability):\n",
        "    \"\"\"\n",
        "    This function decides between selecting a hard triplet or a random triplet based on the hard_triplet_probability.\n",
        "    \"\"\"\n",
        "    if np.random.rand() < hard_triplet_probability:\n",
        "        # With a certain probability, choose the hardest triplet\n",
        "        return get_hard_triplet(sample, negatives, base_model, clf_model)\n",
        "    else:\n",
        "        # Otherwise, choose a random triplet\n",
        "        return get_random_triplet(sample, negatives)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLXLb3g8rOrA"
      },
      "source": [
        "#### Training on Semi-Hard Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJqrVYYqrsfn"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "patience = 10\n",
        "previous_loss = float('inf')\n",
        "\n",
        "hard_triplet_probability_start=0.5\n",
        "hard_triplet_probability_end=0.8\n",
        "\n",
        "early_stopping_2 = EarlyStopping(monitor='loss', patience=patience, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWfTVmvZrxav",
        "outputId": "c68edc37-245f-46e7-fec2-c9fd18b3b52f"
      },
      "outputs": [],
      "source": [
        "# Initial probability of selecting a hard triplet\n",
        "triplet_select_probability = hard_triplet_probability_start\n",
        "\n",
        "# Iterate over each epoch\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  # Initialize empty lists for anchor, positive, negative samples and labels\n",
        "  anchor_samples = []\n",
        "  positive_samples = []\n",
        "  negative_samples = []\n",
        "  labels = []\n",
        "\n",
        "  # Iterate over triplet samples\n",
        "  for _, sample in triplet_sample_candidates.items():\n",
        "    # Get the anchor, positive, negative samples\n",
        "    anchor, positive, negative = get_triplet(sample, negative_vectors_dict, base_network, clf_network, triplet_select_probability)\n",
        "    # Add samples to their respective lists\n",
        "    anchor_samples.append(anchor)\n",
        "    positive_samples.append(positive)\n",
        "    negative_samples.append(negative)\n",
        "    labels.append(0)\n",
        "\n",
        "  # Convert lists to numpy arrays\n",
        "  anchor_samples = np.array(anchor_samples)\n",
        "  positive_samples = np.array(positive_samples)\n",
        "  negative_samples = np.array(negative_samples)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  # Train the model on current epoch's data\n",
        "  siamese_model.fit([anchor_samples, positive_samples, negative_samples], labels,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping_2, cp_save])\n",
        "\n",
        "  # Gradually increase the probability of choosing a hard triplet\n",
        "  triplet_select_probability += (hard_triplet_probability_end - hard_triplet_probability_start) / num_epochs\n",
        "\n",
        "  # Uncomment the following section for Early Stopping\n",
        "  # Check if current epoch is a 'patience' epoch\n",
        "  if epoch % patience == 0 and epoch != 0:\n",
        "    current_loss = siamese_model.history.history['loss'][-1]\n",
        "    # Check if loss is increasing or constant, if yes, then stop training\n",
        "    if current_loss >= previous_loss:\n",
        "      print(\"Early stopping triggered. Stopping training.\")\n",
        "      break\n",
        "    else:\n",
        "      # Update previous loss with current loss\n",
        "      previous_loss = current_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9k4beR9tu4o"
      },
      "source": [
        "# Inference and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdy5TBdrtxTV"
      },
      "outputs": [],
      "source": [
        "def generate_concatenated_vectors(data, base_network):\n",
        "  concatenated_vectors = []\n",
        "  labels = []\n",
        "\n",
        "  for k, v in tqdm(data.items(), total=len(data)):\n",
        "    # Process known vectors\n",
        "    known_feature_vectors = base_network.predict(np.array(v['known']), verbose=0)\n",
        "\n",
        "    # Process unknown vectors\n",
        "    unknown_feature_vectors = base_network.predict(np.array(v['unknown']), verbose=0)\n",
        "\n",
        "    # Compute the average feature vector\n",
        "    author_representation = np.mean(known_feature_vectors, axis=0)\n",
        "    unknown_representation = np.mean(unknown_feature_vectors, axis=0)\n",
        "\n",
        "    concate_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n",
        "\n",
        "    concatenated_vectors.append(concate_vec)\n",
        "    labels.append(v['label'])\n",
        "\n",
        "  return np.array(concatenated_vectors), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AxJfKnxuRHm",
        "outputId": "9b41968e-61f9-4dc6-9d62-ee0e62dbbee0"
      },
      "outputs": [],
      "source": [
        "# Build train siamese_embedding dataset\n",
        "train_siamese_vec, train_siamese_labels = generate_concatenated_vectors(train_data, base_network)\n",
        "\n",
        "# Build val siamese_embedding dataset\n",
        "val_siamese_vec, val_siamese_labels = generate_concatenated_vectors(val_data, base_network)\n",
        "\n",
        "# Build test siamese_embedding dataset\n",
        "test_siamese_vec, test_siamese_labels = generate_concatenated_vectors(test_data, base_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYLuWkM_QbYR"
      },
      "outputs": [],
      "source": [
        "clf_network.compile(optimizer='adam',\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy', tf.keras.metrics.AUC()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53b-IY7I2xh-",
        "outputId": "b70346fe-f58b-447b-9e73-6b04053a4741"
      },
      "outputs": [],
      "source": [
        "clf_early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
        "clf_history = clf_network.fit(train_siamese_vec, train_siamese_labels,\n",
        "                              epochs=1000,\n",
        "                              verbose=1,\n",
        "                              validation_data = (val_siamese_vec, val_siamese_labels),\n",
        "                              callbacks=[clf_early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sehqxBD07HCR",
        "outputId": "69ed0d1c-d74e-469f-c032-9c43f5522919"
      },
      "outputs": [],
      "source": [
        "res = clf_network.evaluate(test_siamese_vec, test_siamese_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40B63860CvLq",
        "outputId": "dba08f08-a775-446f-a493-d4c0ca49c4f8"
      },
      "outputs": [],
      "source": [
        "res = clf_network.evaluate(test_siamese_vec, test_siamese_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C89dzA6bxVsf"
      },
      "source": [
        "# Calculate Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRskuFpyxYFx"
      },
      "outputs": [],
      "source": [
        "def calculate_score(y_predict, y_true):\n",
        "    n = len(y_predict)\n",
        "    n_correct = 0\n",
        "    n_unknown = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        if y_predict[i] > 0.5:\n",
        "            prediction = 1\n",
        "        elif y_predict[i] < 0.5:\n",
        "            prediction = 0\n",
        "        else:\n",
        "            n_unknown += 1\n",
        "            continue\n",
        "\n",
        "        if prediction == y_true[i]:\n",
        "            n_correct += 1\n",
        "\n",
        "    c_1 = (n_correct + (n_unknown * n_correct / n)) / n\n",
        "    auc = tf.keras.metrics.AUC()(y_true, y_predict)\n",
        "    score = auc.numpy() * c_1\n",
        "\n",
        "    return c_1, auc.numpy(), score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdsN95EnxZhj",
        "outputId": "7d9d4627-f80f-4af7-e564-81860a51ac1f"
      },
      "outputs": [],
      "source": [
        "nn_pred = clf_network.predict(test_siamese_vec)\n",
        "c_1, auc, score = calculate_score(nn_pred, test_siamese_labels)\n",
        "\n",
        "print(\"C@1:\", round(c_1, 3))\n",
        "print(\"AUC:\", round(auc, 3))\n",
        "print(\"Final Score:\", round(score, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_emTPtXkcCq"
      },
      "source": [
        "# Cleanup\n",
        "Optional: only run if you need to clean up the directory to re-run tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb-o7Q9ykauz"
      },
      "outputs": [],
      "source": [
        "def deep_clean(path, inc_root = False):\n",
        "  \"\"\"Deletes the contents of a dir, and (optionally) the dir itself\"\"\"\n",
        "\n",
        "  if not os.path.isdir(path):\n",
        "    print(f\"Invalid path: {path}\")\n",
        "    return\n",
        "\n",
        "  if len(os.listdir(path)):\n",
        "    for root, dirs, files in os.walk(path, topdown=False):\n",
        "      for file in files:\n",
        "        os.remove(os.path.join(root, file))\n",
        "      for dir in dirs:\n",
        "        os.rmdir(os.path.join(root, dir))\n",
        "  else:\n",
        "    print(f\"{path} is already empty!\")\n",
        "\n",
        "  if inc_root:\n",
        "    os.rmdir(path)\n",
        "    print(f\"Removed {path} and contents\")\n",
        "  else:\n",
        "    print(f\"Removed contents of {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oMzu5OzEDfc"
      },
      "outputs": [],
      "source": [
        "print(\"WARNING: This will remove all SiameseNet weights, Word2Vec saved models and downloaded nltk_data!\")\n",
        "if input(\"Are you sure? \").lower().strip() == \"yes\":\n",
        "  # nltk downloads, if they're in the working directory\n",
        "  deep_clean(\"nltk_data\", True)\n",
        "\n",
        "  # checkpoint information\n",
        "  deep_clean(\"model_weights\", False)\n",
        "\n",
        "  # saved Word2Vec model, if it exists\n",
        "  try:\n",
        "    os.remove(\"Word2Vec.model\")\n",
        "    print(\"Removed Word2Vec.model\")\n",
        "  except FileNotFoundError:\n",
        "    print(\"Invalid file: Word2Vec.model\")\n",
        "\n",
        "else:\n",
        "  print(\"Abort\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVadtDLybBiq"
      },
      "source": [
        "# Production Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1erptilSa5wM"
      },
      "source": [
        "### Saving/Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmdVAnu2fTCL"
      },
      "outputs": [],
      "source": [
        "# Save Gensim Word2Vec model\n",
        "DEFAULT_W2V_SAVEFILE = \"Word2Vec.model\"\n",
        "word2vec_model.save(DEFAULT_W2V_SAVEFILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2D04KHSbFk9"
      },
      "outputs": [],
      "source": [
        "# Load Word2Vec Model\n",
        "W2V_SAVEFILE = \"Word2Vec.model\"\n",
        "loadW2v = lambda path : gensim.models.Word2Vec.load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdgCPZNkbeGZ"
      },
      "outputs": [],
      "source": [
        "# Function to load a new SiameseNet model from existing weights\n",
        "DEFAULT_CHECKPOINTS_PATH = \"model_weights/cp.ckpt\"\n",
        "DEFAULT_EMBEDDING_DIM = (323,)   # Based on value from training data, but not stored with the weights\n",
        "\n",
        "def loadSiameseNet(checkpoint_path, embedding_dim = DEFAULT_EMBEDDING_DIM):\n",
        "  # Redefine the model\n",
        "  base_network = create_base_network(embedding_dim)\n",
        "  clf_network = create_clf_network(base_network.output_shape[1]*2)\n",
        "\n",
        "  siamese_model = SiameseNet(base_network, clf_network)\n",
        "\n",
        "  # Assembled the model\n",
        "  siamese_model.compile(optimizer='adam',\n",
        "                      loss=customer_loss)\n",
        "\n",
        "  # Locate the checkpoint file, and ensure it is the right filetype\n",
        "  if os.path.basename(checkpoint_path) == \"cp.ckpt\":\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "  else:\n",
        "    raise ValueError(f\"Model Checkpoint '{checkpoint_path}' does not exist!\")\n",
        "\n",
        "  latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  siamese_model.load_weights(latest)\n",
        "\n",
        "  # Compile the clf network\n",
        "  clf_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "  return siamese_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-zzBBi_-QaO"
      },
      "outputs": [],
      "source": [
        "def save_profile(name, profile_dir = \"models\", weights_dir = \"model_weights\", w2v_save = \"Word2Vec.model\"):\n",
        "  # Ensure destination exists\n",
        "  if not os.path.isdir(profile_dir):\n",
        "    try:\n",
        "      os.mkdir(profile_dir)\n",
        "    except FileExistsError:\n",
        "      os.remove(profile_dir)\n",
        "      os.mkdir(profile_dir)\n",
        "    print(f\"Created {profile_dir}\")\n",
        "\n",
        "  save_dir = os.path.join(profile_dir, name)\n",
        "  try:\n",
        "    os.mkdir(save_dir)\n",
        "    print(f\"Created {save_dir}\")\n",
        "  except FileExistsError:\n",
        "    if not os.path.isdir(save_dir):\n",
        "      os.remove(save_dir)\n",
        "      os.mkdir(save_dir)\n",
        "    else:\n",
        "      deep_clean(save_dir, False)\n",
        "\n",
        "  # Store weights\n",
        "  weights_save = os.path.join(save_dir, \"weights\")\n",
        "  os.mkdir(weights_save)\n",
        "  for file in os.scandir(weights_dir):\n",
        "    if not os.path.isfile(file): continue\n",
        "\n",
        "    nfile = os.path.join(weights_save, os.path.basename(file))\n",
        "    os.rename(file, os.path.join(weights_save, os.path.basename(file)))\n",
        "    print(f\"{file.name}->{nfile}\")\n",
        "\n",
        "  # Store Word2Vec\n",
        "  os.rename(w2v_save, os.path.join(save_dir, os.path.basename(w2v_save)))\n",
        "  print(f\"{w2v_save}->{os.path.join(save_dir, os.path.basename(w2v_save))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-oshHRfDBDA",
        "outputId": "0f35df2d-8e3e-4f1b-d091-c5a66ee403cb"
      },
      "outputs": [],
      "source": [
        "# Save a profile with a custom name\n",
        "save_profile(input(\"profile name? \").lower().strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mujo54RggYRc"
      },
      "source": [
        "### Proof-of-concept use functions\n",
        "Functions that run the model on a single profile at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOoL-jotg4I1"
      },
      "outputs": [],
      "source": [
        "def vectorize_set(known,unknown,w2v_model):\n",
        "    \"\"\"Converts a single set of texts (instead of the entire corpus) to vectors\"\"\"\n",
        "    vectors = {\n",
        "        'known': get_vectors(known, w2v_model),\n",
        "        'unknown': get_vectors(unknown, w2v_model)\n",
        "        # 'label' not included since it's not relevant outside of training\n",
        "\t  }\n",
        "    return vectors\n",
        "\n",
        "def concatenate_vector_set(vectors, base_network):\n",
        "    known_feature_vectors = base_network(np.array(vectors['known']))\n",
        "    unknown_feature_vectors = base_network(np.array(vectors['unknown']))\n",
        "\n",
        "    author_representation = np.mean(known_feature_vectors, axis=0)\n",
        "    unknown_representation = np.mean(unknown_feature_vectors, axis=0)\n",
        "\n",
        "    concate_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n",
        "    return concate_vec\n",
        "\n",
        "def predict_once(path, w2v_model, base_network, clf_network) -> float:\n",
        "    files = []\n",
        "    for f in os.listdir(path):\n",
        "        files.append(f\"{path}/{f}\")\n",
        "\n",
        "    known_data, unknown_data = extract_text_from_files(files)\n",
        "\n",
        "    vectors = vectorize_set(known_data, unknown_data, w2v_model)\n",
        "    concats = concatenate_vector_set(vectors, base_network)\n",
        "\n",
        "    prediction : tf.Tensor = clf_network(np.array([concats]))\n",
        "\n",
        "    # Convert the Tensor to a numpy array and flatten, as output shape will be (1,1)\n",
        "    return prediction.numpy()[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2vec_model = loadW2v(W2V_SAVEFILE)\n",
        "siamese_model : SiameseNet = loadSiameseNet(DEFAULT_CHECKPOINTS_PATH)\n",
        "clf_network = siamese_model.clf\n",
        "base_network = siamese_model.base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayf2N9dLnB4p"
      },
      "outputs": [],
      "source": [
        "# Single point test of values from freshly generated model vs. loaded one\n",
        "def load_and_test(path):\n",
        "  # Using the base_network and clf_network variables from the generation\n",
        "  print(predict_once(path, word2vec_model, base_network, clf_network))\n",
        "\n",
        "  # Pulling those same variables from the siamese_model object (still from the freshly generated model)\n",
        "  print(predict_once(path, word2vec_model, siamese_model.base, siamese_model.clf))\n",
        "\n",
        "  # Load both models\n",
        "  w2v_loaded = loadW2v(W2V_SAVEFILE)\n",
        "  siamese_loaded = loadSiameseNet(DEFAULT_CHECKPOINTS_PATH)\n",
        "  print(predict_once(path, w2v_loaded, siamese_loaded.base, siamese_model.clf))\n",
        "\n",
        "load_and_test(\"data/test_data/EE002\")\n",
        "# All values should be identical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiNv3hmluyJt"
      },
      "source": [
        "### Self-Contained Classes\n",
        "Handles both loading and running the models, with all required functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Extra utility functions for use with classes ###\n",
        "def strip_text(data):\n",
        "\tif type(data) is str: data = data.splitlines()\n",
        "\n",
        "\ttext = []\n",
        "\tfor line in data:\n",
        "\t\tif type(line) is not str: line = str(line)\n",
        "\t\tcleaned = line.strip().lstrip(\"\\ufeff\")\n",
        "\t\ttext.append(cleaned)\n",
        "\n",
        "\treturn text\n",
        "\n",
        "def setupNltk(path = f\"{os.curdir}/nltk_data\") -> None:\n",
        "\t\"\"\"Set up the NLTK package path and downloads datapacks (if required)\"\"\"\n",
        "\tnltk.data.path = [ path ]\n",
        "\tnltk.download([\"punkt\", \"stopwords\",\"wordnet\"], download_dir=nltk.data.path[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dHkb5wIuwFS"
      },
      "outputs": [],
      "source": [
        "class StyloNet:\n",
        "\t\"\"\"Contains the whole stylometry model, functions score, score_multi, predict and predict_multi\n",
        "\t\tcan be called to run the stylometry model on a text.\n",
        "\t\t\n",
        "\t\tInput format for predictions is the following:\n",
        "\t\t\ttexts = {\n",
        "\t\t\t\t'known': list[str] (known text(s))\n",
        "\t\t\t\t'unknown': list[str] (unknown text(s))\n",
        "\t\t\t}\n",
        "\t\tAll lists for texts can be multi-dimensional, as long as it only ends in strings\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, working_dir:str = os.curdir, valid_threshold:float = 0.5,\n",
        "\t\t\t\t\tmodel_checkpoints:str = \"model_weights\", w2v_save:str = \"Word2Vec.model\", nltk_path:str = \"nltk_data\"):\n",
        "\t\t\n",
        "\t\t# Most args can be left at their default values, as long as working_dir is correct the model should work.\n",
        "\t\tif not os.path.isdir(working_dir):\n",
        "\t\t\traise OSError(f\"Working dir for StyloNet does not exist: {working_dir}\")\n",
        "\t\t\n",
        "\t\tsetupNltk(os.path.join(working_dir, nltk_path))\n",
        "\n",
        "\t\t# Load Word2Vec model\n",
        "\t\tself.word2vec = loadW2v(os.path.join(working_dir, w2v_save))\n",
        "\t\t\n",
        "\t\t# Redefine and load the SiameseNet model from checkpoints\n",
        "\t\tself.siamese_model = buildSiameseNet(os.path.join(working_dir, model_checkpoints))\n",
        "\t\tself.base_network, self.clf_network = self.siamese_model.base, self.siamese_model.clf\n",
        "\n",
        "\t\t# Save validiy threshold for making predictions\n",
        "\t\tself.threshold = valid_threshold\n",
        "\n",
        "\tdef _vectorize(self,text : dict):\n",
        "\t\tvectors = {\n",
        "\t\t\t'known': get_vectors(text['known'],self.word2vec),\n",
        "\t\t\t'unknown': get_vectors(text['unknown'],self.word2vec)\n",
        "\t\t}\n",
        "\t\treturn vectors\n",
        "\n",
        "\tdef _vectorize_multi(self, texts : list[dict]):\n",
        "\t\tvectors = []\n",
        "\t\tfor text in texts:\n",
        "\t\t\tvectors.append(self._vectorize(text))\n",
        "\t\treturn vectors\n",
        "\n",
        "\tdef _concatenate(self, vectors : dict):\n",
        "\t\tknown_feature_vectors = self.base_network(np.array(vectors['known']))\n",
        "\t\tunknown_feature_vectors = self.base_network(np.array(vectors['unknown']))\n",
        "\n",
        "\t\tauthor_representation = np.mean(known_feature_vectors, axis=0)\n",
        "\t\tunknown_representation = np.mean(unknown_feature_vectors, axis=0)\n",
        "\n",
        "\t\tconcat_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n",
        "\t\treturn concat_vec\n",
        "\n",
        "\tdef _concatenate_multi(self, vectors : list[dict]):\n",
        "\t\tconcats = []\n",
        "\t\tfor vec in vectors:\n",
        "\t\t\tconcats.append(self._concatenate(vec))\n",
        "\t\treturn np.array(concats)\n",
        "\n",
        "\t### Interface Functions ###\n",
        "\tdef score(self, texts : dict) -> float:\n",
        "\t\t\"\"\"Run the model and return the similarity score as a decimal\"\"\"\n",
        "\t\tif len(texts['unknown']) == 0: return 0  # Incase of empty unknown set\n",
        "\n",
        "\t\tvectors = self._vectorize(texts['known'], texts['unknown'])\n",
        "\t\tconcats = self._concatenate(vectors)\n",
        "\t\t\n",
        "\t\tprediction : tf.Tensor = self.clf_network(np.array([concats]))\n",
        "\t\tif prediction.shape != (1,1): raise ValueError(\"Result incorrect shape!\")\n",
        "\t\t\n",
        "\t\t# Convert to numpy array and flatten, as output shape will be (1,1)\n",
        "\t\treturn prediction.numpy()[0][0]\n",
        "\n",
        "\tdef score_batch(self, texts: list|dict) -> list[float]|dict:\n",
        "\t\t\"\"\"Calculate score over a list or dictionary of texts and return a list/dict of the results\"\"\"\n",
        "\t\ttexts = texts.values() if type(texts) is dict else texts\n",
        "\n",
        "\t\tvectors = self._vectorize_multi(texts)\n",
        "\t\tconcats = self._concatenate_multi(vectors)\n",
        "\n",
        "\t\tpredicts = self.clf_network.predict(concats, verbose=0)\n",
        "\t\t\n",
        "\t\tif type(texts) is dict:\n",
        "\t\t\tresults = {}\n",
        "\t\t\tfor i, key in enumerate(texts.keys()):\n",
        "\t\t\t\t# Unpack np.array and match the predictions up to the keys of the original dictionary\n",
        "\t\t\t\tresults[key] = predicts[i][0]\n",
        "\t\telse:\n",
        "\t\t\tresults = []\n",
        "\t\t\tfor val in predicts:\n",
        "\t\t\t\t# Unpack the nested np.array to convert to a basic array of results\n",
        "\t\t\t\tresults.append(val[0])\n",
        "\t\t\n",
        "\t\treturn results\n",
        "\n",
        "\tdef predict(self, texts: dict) -> bool:\n",
        "\t\t\"\"\"Calculate score and return a prediction based on the predetermined threshold, returns a boolean result\"\"\"\n",
        "\t\treturn self.score(texts) >= self.threshold\n",
        "\n",
        "\tdef predict_batch(self, texts: list|dict) -> list[bool]|dict:\n",
        "\t\t\"\"\"Run predict over a list/dict of texts and return the result with a boolean result\"\"\"\n",
        "\t\tscores = self.score_batch(texts)\n",
        "\t\tpred = lambda s : s >= self.threshold\n",
        "\n",
        "\t\tif type(texts) is dict:\n",
        "\t\t\tresults = {}\n",
        "\t\t\tfor key, val in scores.items():\n",
        "\t\t\t\tresults[key] = pred(val)\n",
        "\t\telse:\n",
        "\t\t\tresults = []\n",
        "\t\t\tfor val in scores:\n",
        "\t\t\t\tresults.append(pred(val))\n",
        "\t\t\n",
        "\t\treturn results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StyloInst:\n",
        "    \"\"\"A single instance of the StyloNet class, intended to be used to store profile variables\n",
        "        but currently unnecessary\"\"\"\n",
        "    def __init__(self, parent : StyloNet):\n",
        "        self.parent = parent\n",
        "        self.known = []\n",
        "        self.unknown = []\n",
        "        self.lastResult = None\n",
        "\t\n",
        "    def addKnown(self, data):\n",
        "        self.known.append(strip_text(data))\n",
        "    \n",
        "    def addUnknown(self, data):\n",
        "        self.unknown.append(strip_text(data))\n",
        "    \n",
        "    def predict(self):\n",
        "        data = { 'known' : self.known, 'unknown' : self.unknown }\n",
        "        result = self.parent.predict(data)\n",
        "        self.lastResult = result\n",
        "        return result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TgkfIYdVgrY3"
      },
      "source": [
        "### Testing and Verification\n",
        "Verifies the output of the single-case functions to ensure they match the output of the original code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "### Test the class defined above to ensure it generates the same output on all datasets ###\n",
        "def verifyResults(loaded, generated):\n",
        "    for i in range(len(loaded)):\n",
        "        if loaded[i] != generated[i][0]:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Run predictions over all 3 precompiled data-sets using the generated model\n",
        "generated_test_results = clf_network.predict(test_siamese_vec)\n",
        "generated_train_results = clf_network.predict(train_siamese_vec)\n",
        "generated_val_results = clf_network.predict(val_siamese_vec)\n",
        "\n",
        "# Run predictions over the same dataset using the loaded model\n",
        "loaded_model = StyloNet()\n",
        "loaded_test_results = loaded_model.predict_batch(test_corpus)\n",
        "loaded_train_results = loaded_model.predict_batch(train_corpus)\n",
        "loaded_val_results = loaded_model.predict_batch(val_corpus)\n",
        "\n",
        "# Verify results\n",
        "print(f\"Test Results: {verifyResults(loaded_test_results, generated_test_results)}\")\n",
        "print(f\"Train Results: {verifyResults(loaded_train_results, generated_train_results)}\")\n",
        "print(f\"Val Results: {verifyResults(loaded_val_results, generated_val_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to ./nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "### Load StyloNet from library ###\n",
        "from importlib import reload\n",
        "\n",
        "try:\n",
        "    reload(Stylometry)\n",
        "except NameError:\n",
        "\timport Stylometry\n",
        "\n",
        "loaded_model = Stylometry.StyloNet()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RkBeR4eOPEm2",
        "uFDDT_rmRokt",
        "THrYBCxhimox",
        "mmK7NtP31jdn",
        "FC2AmF8om5V4",
        "M9k4beR9tu4o",
        "YkYSjARCxBZ0",
        "C89dzA6bxVsf"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
