{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt4R6YMSF7sG"
      },
      "source": [
        "# Set Up\n",
        "Set up the Google Colab environment and import dependent libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rlYEd4wF2tU",
        "outputId": "93c35f8f-bea0-4d97-8f4d-36f03417e178"
      },
      "outputs": [],
      "source": [
        "#Loading data from Google drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  os.chdir(\"/content/drive/My Drive/PAN14_Code\")\n",
        "  RUNNING_COLAB = True\n",
        "except ImportError:\n",
        "  print(\"I have a sneaking suspicion that I'm not running on Google Colab\")\n",
        "  RUNNING_COLAB = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqfYit_PWqhW"
      },
      "outputs": [],
      "source": [
        "# Lambda to print module versions\n",
        "ver = lambda module : print(f\"{module.__name__}=={module.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj0ew-WfF-Fv",
        "outputId": "d250a9f1-6768-46e3-c331-a70bf30d531d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import itertools\n",
        "from collections import Counter\n",
        "ver(np)\n",
        "\n",
        "#pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download([\"punkt\", \"stopwords\",\"wordnet\"])\n",
        "ver(nltk)\n",
        "\n",
        "# Uncomment to suppress tensorflow output, if need\n",
        "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "#pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "ver(tf)\n",
        "\n",
        "#pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import common_texts\n",
        "ver(gensim)\n",
        "\n",
        "#pip install scikit-learn\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "from collections import defaultdict\n",
        "\n",
        "#pip install networkx\n",
        "import networkx as nx\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlretrieve\n",
        "ver(nx)\n",
        "\n",
        "#pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGXneK8UW8EK",
        "outputId": "b1358140-6037-4b23-c8df-5e4477876886"
      },
      "outputs": [],
      "source": [
        "def nltk_setup(path = None):\n",
        "  \"\"\"Initialize and download the right modules\"\"\"\n",
        "  if type(path) is None:\n",
        "    # No change to default path (defaults to user home directory)\n",
        "    nltk.download([\"punkt\", \"stopwords\",\"wordnet\"])\n",
        "  else:\n",
        "    # Change default path\n",
        "    nltk.data.path = [ path ]\n",
        "    nltk.download([\"punkt\", \"stopwords\",\"wordnet\"], download_dir=nltk.data.path[0])\n",
        "\n",
        "nltk_setup(f\"{os.path.curdir}/nltk_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82RifHrL0W8S",
        "outputId": "2c21ec79-e0ff-4b9c-cdab-a97c12dd21c8"
      },
      "outputs": [],
      "source": [
        "def print_colab_stats():\n",
        "  # GPU info\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)\n",
        "\n",
        "  # Memory Info\n",
        "  from psutil import virtual_memory\n",
        "  ram_gb = virtual_memory().total / 1e9\n",
        "  print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "  if ram_gb < 20:\n",
        "    print('Not using a high-RAM runtime')\n",
        "  else:\n",
        "    print('You are using a high-RAM runtime!')\n",
        "\n",
        "if RUNNING_COLAB: print_colab_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mepv72EIF_kq"
      },
      "source": [
        "# Data Process\n",
        "Read the data in and save it in the dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiwPGxTzI5Gl"
      },
      "outputs": [],
      "source": [
        "def get_data_directory_path(subdirectory):\n",
        "    return os.path.join('data', subdirectory)\n",
        "\n",
        "\n",
        "def get_json_file_path(data_directory, file_name):\n",
        "    return os.path.join(data_directory, file_name)\n",
        "\n",
        "\n",
        "def read_json_file(file_path):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_text_from_files(file_paths):\n",
        "    known_text, unknown_text = [], []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        text_lines = []\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                cleaned_line = line.strip().lstrip(\"\\ufeff\")\n",
        "                text_lines.append(cleaned_line)\n",
        "        if 'unknown' in file_path:\n",
        "            unknown_text.append(text_lines)\n",
        "        else:\n",
        "            known_text.append(text_lines)\n",
        "\n",
        "    return known_text, unknown_text\n",
        "\n",
        "\n",
        "def build_corpus(data_directory, content_data, label_data):\n",
        "    corpus = {}\n",
        "\n",
        "    for index in tqdm(range(len(content_data['problems']))):\n",
        "        problem_file_paths = glob.glob(os.path.join(data_directory, content_data['problems'][index], '*'))\n",
        "\n",
        "        if not problem_file_paths:\n",
        "            continue\n",
        "\n",
        "        known_text, unknown_text = extract_text_from_files(problem_file_paths)\n",
        "        label = 1 if label_data['problems'][index]['answer'] == 'Y' else 0\n",
        "\n",
        "        corpus[index] = {\n",
        "            'known': known_text,\n",
        "            'unknown': unknown_text,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVzAw15TL8CA"
      },
      "outputs": [],
      "source": [
        "# Get data path\n",
        "train_data_directory = get_data_directory_path('train_data')\n",
        "validation_data_directory = get_data_directory_path('val_data')\n",
        "test_data_directory = get_data_directory_path('test_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SebVk4WpI9c2"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "train_content = read_json_file(get_json_file_path(train_data_directory, 'contents.json'))\n",
        "train_labels = read_json_file(get_json_file_path(train_data_directory, 'truth.json'))\n",
        "# # Val\n",
        "#validation_content = read_json_file(get_json_file_path(validation_data_directory, 'contents.json'))\n",
        "#validation_labels = read_json_file(get_json_file_path(validation_data_directory, 'truth.json'))\n",
        "# Test\n",
        "test_content = read_json_file(get_json_file_path(test_data_directory, 'contents.json'))\n",
        "test_labels = read_json_file(get_json_file_path(test_data_directory, 'truth.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhTF9bRPL7Ac",
        "outputId": "47bd2a53-40e1-4994-ff45-d2300415b79b"
      },
      "outputs": [],
      "source": [
        "# Get train corpus\n",
        "train_corpus = build_corpus(train_data_directory, train_content, train_labels)\n",
        "# # Get val corpus\n",
        "#val_corpus = build_corpus(validation_data_directory, validation_content, validation_labels)\n",
        "# Get test corpus\n",
        "test_corpus = build_corpus(test_data_directory, test_content, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the training data into training and validation\n",
        "def split_data_into_train_and_val(data_dict, test_size=0.2, random_state=42):\n",
        "    document_ids, labels = zip(*[(doc_id, data['label']) for doc_id, data in data_dict.items()])\n",
        "\n",
        "    train_ids, val_ids, train_labels, val_labels = train_test_split(document_ids, labels, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    train_data = {doc_id: data_dict[doc_id] for doc_id in train_ids}\n",
        "    validation_data = {doc_id: data_dict[doc_id] for doc_id in val_ids}\n",
        "\n",
        "    return train_data, validation_data\n",
        "\n",
        "train_corpus, val_corpus = split_data_into_train_and_val(data_dict=train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkBeR4eOPEm2"
      },
      "source": [
        "# Train Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wuFlKWaRJ1c"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess a given text by tokenizing, removing punctuation and numbers,\n",
        "    removing stop words, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        list: The preprocessed text as a list of tokens.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    table = str.maketrans('', '', string.punctuation + string.digits)\n",
        "    tokens = [word.translate(table) for word in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if (not word in stop_words) and (word != '')]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def train_word2vec_model(data, vector_size):\n",
        "    \"\"\"\n",
        "    Train a word2vec model using the given data.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The data to use for training the model.\n",
        "        vector_size (int): The size of the word vectors in the model.\n",
        "\n",
        "    Returns:\n",
        "        gensim.models.Word2Vec: The trained word2vec model.\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "\n",
        "    # Process all articles in the data\n",
        "    for articles in tqdm(data.values(), total=len(data)):\n",
        "        all_articles = []\n",
        "        all_articles.extend(articles['known'])\n",
        "        all_articles.extend(articles['unknown'])\n",
        "\n",
        "        for article in all_articles:\n",
        "            for line in article:\n",
        "                text = line.strip()\n",
        "                tokens = preprocess_text(text)\n",
        "                corpus.append(tokens)\n",
        "\n",
        "    # Train the word2vec model\n",
        "    word2vec_model = gensim.models.Word2Vec(vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "    word2vec_model.build_vocab(corpus)\n",
        "    word2vec_model.train(corpus, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
        "\n",
        "    return word2vec_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVvsP3-_RS5_"
      },
      "outputs": [],
      "source": [
        "# Size of word vectors in the word2vec model\n",
        "w2v_vector_size = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejtm_oXrRP4-",
        "outputId": "0909a260-3371-4864-8a10-e179c339b56d"
      },
      "outputs": [],
      "source": [
        "# Train a word2vec model using the training corpus\n",
        "word2vec_model = train_word2vec_model(train_corpus, w2v_vector_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFDDT_rmRokt"
      },
      "source": [
        "# Vectorize Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDsYcGE-TyUc"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_vector(texts, model, vector_size):\n",
        "    \"\"\"\n",
        "    Convert a list of texts into their corresponding word2vec vectors\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for text in texts:\n",
        "        words = preprocess_text(text)\n",
        "        vector = np.sum([model.wv[word] for word in words if word in model.wv], axis=0)\n",
        "        word_count = np.sum([word in model.wv for word in words])\n",
        "        if word_count != 0:\n",
        "            vector /= word_count\n",
        "        else:\n",
        "          vector = np.zeros(vector_size)\n",
        "        vectors.append(vector)\n",
        "    return vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F6DWIL9UZIw"
      },
      "outputs": [],
      "source": [
        "def count_punctuations(texts):\n",
        "  \"\"\"\n",
        "  Count the frequency of different punctuations in the texts\n",
        "  \"\"\"\n",
        "  # Define punctuations to count\n",
        "  punctuations = set(['.', ',', ';', ':', '!', '?', '-', '(', ')', '\\\"', '\\'', '`', '/'])\n",
        "\n",
        "  # Initialize dictionary to count punctuations\n",
        "  punctuations_count = {p: 0 for p in punctuations}\n",
        "\n",
        "  # Count punctuations in text_list\n",
        "  for text in texts:\n",
        "      for char in text:\n",
        "          if char in punctuations:\n",
        "              punctuations_count[char] += 1\n",
        "\n",
        "  # Return list of punctuation counts\n",
        "  return list(punctuations_count.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_IQDJiqUX7C"
      },
      "outputs": [],
      "source": [
        "def analyze_sentence_lengths(sentences):\n",
        "  \"\"\"\n",
        "  Analyze the lengths of sentences\n",
        "  \"\"\"\n",
        "  sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
        "  average_length = np.mean(sentence_lengths)\n",
        "  count_over_avg = np.sum([length > average_length for length in sentence_lengths])\n",
        "  count_under_avg = np.sum([length < average_length for length in sentence_lengths])\n",
        "  count_avg = len(sentence_lengths) - count_over_avg - count_under_avg\n",
        "\n",
        "  return [count_over_avg, count_under_avg, count_avg, average_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pevRcCjGUWyz"
      },
      "outputs": [],
      "source": [
        "def analyze_words(texts):\n",
        "    \"\"\"\n",
        "    Analyze the words used in the texts\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for text in texts:\n",
        "        tokenized = word_tokenize(text.lower())\n",
        "        processed = [lemmatizer.lemmatize(word) for word in tokenized if word not in stop_words]\n",
        "        words += processed\n",
        "    word_freq = nltk.FreqDist(words)\n",
        "    rare_count = np.sum([freq <= 2 for word, freq in word_freq.items()])\n",
        "    long_count = np.sum([len(word) > 6 for word in words])\n",
        "    word_lengths = [len(word) for word in words]\n",
        "    average_length = np.mean(word_lengths)\n",
        "    count_over_avg = np.sum([length > average_length for length in word_lengths])\n",
        "    count_under_avg = np.sum([length < average_length for length in word_lengths])\n",
        "    count_avg = len(word_lengths) - count_over_avg - count_under_avg\n",
        "    ttr = len(set(words)) / len(words) if words else 0\n",
        "\n",
        "    return [rare_count, long_count, count_over_avg, count_under_avg, count_avg, ttr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoTx1ZxWUUOE"
      },
      "outputs": [],
      "source": [
        "def calculate_style_vector(texts):\n",
        "  \"\"\"\n",
        "  Calculate the style vector of the texts\n",
        "  \"\"\"\n",
        "  punctuation_vec = count_punctuations(texts)     # Punctuations stylistic features\n",
        "  sentence_vec = analyze_sentence_lengths(texts)  # Sentences stylistic features\n",
        "  word_vec = analyze_words(texts)                 # Words stylistic features\n",
        "  word_count = np.sum([len(text.split()) for text in texts])\n",
        "\n",
        "  vector = np.concatenate((punctuation_vec, sentence_vec, word_vec))\n",
        "\n",
        "  return vector / word_count if word_count else vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsJRlqF8gHyO"
      },
      "outputs": [],
      "source": [
        "def get_vectors(texts, w2v_model, vector_size):\n",
        "  res = []\n",
        "  for text in texts:\n",
        "    w2v_vec = np.mean(convert_text_to_vector(text, w2v_model, vector_size), axis=0)\n",
        "    style_vec = calculate_style_vector(text)\n",
        "    res.append(np.concatenate((w2v_vec, style_vec), axis=None))\n",
        "    # res.append(w2v_vec)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEYwCl8yR0D_"
      },
      "outputs": [],
      "source": [
        "def vectorize_text_data(data, w2v_model):\n",
        "  \"\"\"\n",
        "  Build author data from the corpus\n",
        "  \"\"\"\n",
        "  res = {}\n",
        "  for key,val in tqdm(data.items(), total=len(data)):\n",
        "    if len(val['unknown']) == 0:\n",
        "      continue\n",
        "    res[key] = {\n",
        "        'known': get_vectors(val['known'], w2v_model),\n",
        "        'unknown': get_vectors(val['unknown'], w2v_model),\n",
        "        'label': val['label']\n",
        "    }\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP_a-J_xX4Vf",
        "outputId": "d332ef49-f77a-431c-a8e0-9f93a99c9b05"
      },
      "outputs": [],
      "source": [
        "train_data = vectorize_text_data(train_corpus, word2vec_model)\n",
        "val_data = vectorize_text_data(val_corpus, word2vec_model)\n",
        "test_data = vectorize_text_data(test_corpus, word2vec_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THrYBCxhimox"
      },
      "source": [
        "# Build Triplet Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5t503c3ip6H"
      },
      "outputs": [],
      "source": [
        "# Random triplet mining\n",
        "def build_random_triplet_sample(data):\n",
        "  \"\"\"\n",
        "  This function creates random triplet samples from the input data\n",
        "  \"\"\"\n",
        "\n",
        "  keys_list = list(data.keys())\n",
        "  triplet_samples = {}\n",
        "\n",
        "  # Initialize the lists for storing the anchor, positive, and negative samples\n",
        "  anchors, positives, negatives = [], [], []\n",
        "\n",
        "  for key,val in tqdm(data.items(), total=len(data)):\n",
        "    n = len(val['known'])\n",
        "    for i in range(n):\n",
        "      for j in range(i+1, n):\n",
        "        anchors.append(val['known'][i])\n",
        "        positives.append(val['known'][j])\n",
        "        # Get negative sample\n",
        "        while True:\n",
        "          random_key = random.choices(keys_list, k=1)\n",
        "          if random_key != key:\n",
        "            break\n",
        "        random_neg_sample = random.choices(data[random_key[0]]['known'], k=1)\n",
        "        negatives.append(random_neg_sample[0])\n",
        "\n",
        "\n",
        "  # Build triplet sample\n",
        "  for i in range(len(anchors)):\n",
        "    triplet_samples[i] = {\n",
        "        'anchor': anchors[i],\n",
        "        'positive': positives[i],\n",
        "        'negative': negatives[i]\n",
        "    }\n",
        "\n",
        "  return triplet_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu8oSknIi65X",
        "outputId": "a68cd853-52e8-4bb7-adcb-c3cfd8a48f80"
      },
      "outputs": [],
      "source": [
        "random_triplet_samples = build_random_triplet_sample(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8eVVLqAmen1"
      },
      "outputs": [],
      "source": [
        "anchor_data = np.array([data['anchor'] for data in random_triplet_samples.values()])\n",
        "positive_data = np.array([data['positive'] for data in random_triplet_samples.values()])\n",
        "negative_data = np.array([data['negative'] for data in random_triplet_samples.values()])\n",
        "labels_data = np.array([0 for _ in random_triplet_samples.values()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMdu9OrJk35",
        "outputId": "6e7d73a0-ab58-473b-ffa0-2aebf54698f3"
      },
      "outputs": [],
      "source": [
        "val_random_triplet_samples = build_random_triplet_sample(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1Qc49FzJl2K"
      },
      "outputs": [],
      "source": [
        "val_anchor_data = np.array([data['anchor'] for data in val_random_triplet_samples.values()])\n",
        "val_positive_data = np.array([data['positive'] for data in val_random_triplet_samples.values()])\n",
        "val_negative_data = np.array([data['negative'] for data in val_random_triplet_samples.values()])\n",
        "val_labels_data = np.array([0 for _ in val_random_triplet_samples.values()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryJTRnGkmlqm"
      },
      "source": [
        "# Build SiameseNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmK7NtP31jdn"
      },
      "source": [
        "## Model Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovKbyGoM079s"
      },
      "outputs": [],
      "source": [
        "class SiameseNet(tf.keras.Model):\n",
        "    def __init__(self, base_network, clf_network):\n",
        "        super().__init__()\n",
        "        self.base = base_network\n",
        "        self.clf = clf_network\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor = inputs[0]\n",
        "        positive = inputs[1]\n",
        "        negative = inputs[2]\n",
        "\n",
        "        output_anchor = self.base(anchor)\n",
        "        output_positive = self.base(positive)\n",
        "        output_negative = self.base(negative)\n",
        "\n",
        "        # Anchor - Positive\n",
        "        x1 = tf.concat([output_anchor, output_positive], axis=-1)\n",
        "        x1_out = self.clf(x1)\n",
        "\n",
        "        # Anchor - Negative\n",
        "        x2 = tf.concat([output_anchor, output_negative], axis=-1)\n",
        "        x2_out = self.clf(x2)\n",
        "\n",
        "        return (x1_out, x2_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGlosr0W7loe"
      },
      "outputs": [],
      "source": [
        "def create_dense_block(x, units, dropout_rate, l1_reg, l2_reg):\n",
        "    x = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return tf.keras.layers.Dropout(dropout_rate)(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10yA51LpmofZ"
      },
      "outputs": [],
      "source": [
        "# Define the base network\n",
        "def create_base_network(embedding_dim, dropout_rate=0.4, l1_reg=0.001, l2_reg=0.001):\n",
        "    input = tf.keras.layers.Input(shape=embedding_dim)\n",
        "    x = tf.keras.layers.BatchNormalization()(input)\n",
        "\n",
        "    x = create_dense_block(x, 256, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 128, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 64, dropout_rate, l1_reg, l2_reg)\n",
        "\n",
        "    x = tf.keras.layers.Dense(64, activation='linear')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=input, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW83mAPuzCDW"
      },
      "outputs": [],
      "source": [
        "def create_clf_network(input_shape, dropout_rate=0.5, l1_reg=0.003, l2_reg=0.003):\n",
        "    input = tf.keras.layers.Input(shape=(input_shape,))\n",
        "    x = tf.keras.layers.BatchNormalization()(input)\n",
        "\n",
        "    x = create_dense_block(x, 128, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 64, dropout_rate, l1_reg, l2_reg)\n",
        "    x = create_dense_block(x, 32, dropout_rate, l1_reg, l2_reg)\n",
        "\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=input, outputs=x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSgJG94HkwYj"
      },
      "outputs": [],
      "source": [
        "def customer_loss(y_true, y_pred):\n",
        "    AP = y_pred[0]\n",
        "    AN = y_pred[1]\n",
        "\n",
        "    loss = 1.0 - AP + AN\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC2AmF8om5V4"
      },
      "source": [
        "## Construct the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iPD1M5im982"
      },
      "outputs": [],
      "source": [
        "# Define the embedding dimension\n",
        "embedding_dim = anchor_data[0].shape\n",
        "\n",
        "# Create base network\n",
        "base_network = create_base_network(embedding_dim)\n",
        "clf_network = create_clf_network(base_network.output_shape[1]*2)\n",
        "\n",
        "siamese_model = SiameseNet(base_network, clf_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8aeEYUGm__W"
      },
      "outputs": [],
      "source": [
        "input_anchor = tf.keras.layers.Input(shape=embedding_dim)\n",
        "input_positive = tf.keras.layers.Input(shape=embedding_dim)\n",
        "input_negative = tf.keras.layers.Input(shape=embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FijK9xqnCor"
      },
      "outputs": [],
      "source": [
        "# Assemble siameseNet model\n",
        "siamese_model.compile(optimizer='adam',\n",
        "                      loss=customer_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtusEHcYnFAZ"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"model_weights/cp_build.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_save = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                             save_weights_only=True,\n",
        "                                             verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssE3i-v1nJZe"
      },
      "source": [
        "## Load SiameseNet Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oizjc50UnGYr"
      },
      "outputs": [],
      "source": [
        "#latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "#siamese_model.load_weights(latest)\n",
        "# Disabled for now since model loading is implemented under 'Production Use' at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo08aYawnUFS"
      },
      "source": [
        "## Train SiameseNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJAUUc9EnWpF"
      },
      "source": [
        "### Train on Random Triplet Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VgFc5gnnZZa",
        "outputId": "a0bf01f7-869e-4b2a-d1f7-537215205753"
      },
      "outputs": [],
      "source": [
        "# Train siameseNet model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
        "siamese_history = siamese_model.fit([anchor_data, positive_data, negative_data], labels_data,\n",
        "                  epochs=1000,\n",
        "                  validation_data=([val_anchor_data, val_positive_data, val_negative_data], val_labels_data),\n",
        "                  callbacks=[early_stopping, cp_save])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kYqKKoldWObe",
        "outputId": "4f0a83b8-8ccc-44c6-f8e2-8dedf88a0179"
      },
      "outputs": [],
      "source": [
        "loss = siamese_history.history['loss']\n",
        "val_loss = siamese_history.history['val_loss']\n",
        "\n",
        "# Draw\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRB88fJtngeR"
      },
      "source": [
        "### Train on Semi-Hard Triplet Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yktUMRHfrUfQ"
      },
      "source": [
        "#### Semi-Hard Samples Construct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7n3UK1InkZ0"
      },
      "outputs": [],
      "source": [
        "# Build semi-hard triplet sample candidates\n",
        "def build_triplet_sample_candidates(data):\n",
        "  res = {}\n",
        "\n",
        "  keys = []\n",
        "  anchors = []\n",
        "  positives = []\n",
        "\n",
        "  for key,val in tqdm(data.items(), total=len(data)):\n",
        "    n = len(val['known'])\n",
        "    for i in range(n-1):\n",
        "      keys.append(key)\n",
        "      anchors.append(val['known'][i])\n",
        "      positives.append(val['known'][i+1:])\n",
        "\n",
        "  for i in range(len(keys)):\n",
        "    res[i] = {\n",
        "        'key': keys[i],\n",
        "        'anchor': anchors[i],\n",
        "        'positives': positives[i]\n",
        "    }\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3IvyvFBpmYp",
        "outputId": "d6a4b5b3-dd1e-4b3f-9fec-146837e21713"
      },
      "outputs": [],
      "source": [
        "triplet_sample_candidates = build_triplet_sample_candidates(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58rM10jr35k4"
      },
      "outputs": [],
      "source": [
        "def create_negative_vectors_dict(data):\n",
        "    negative_vectors_dict = {}\n",
        "    key_list = list(data.keys())\n",
        "\n",
        "    for key in tqdm(key_list, total=len(key_list)):\n",
        "        negative_vectors_dict[key] = []\n",
        "        for k,v in data.items():\n",
        "            if k != key:\n",
        "                for vec in v['known']:\n",
        "                    negative_vectors_dict[key].append(vec)\n",
        "    return negative_vectors_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvTKNBCw5Hgp",
        "outputId": "8f7bfa7e-8be5-4706-e27b-32ed3896e448"
      },
      "outputs": [],
      "source": [
        "negative_vectors_dict = create_negative_vectors_dict(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGz9M-KEqaAw"
      },
      "outputs": [],
      "source": [
        "def select_random_from_list(input_list):\n",
        "    \"\"\"\n",
        "    Selects a random item from a list.\n",
        "    \"\"\"\n",
        "    return input_list[np.random.randint(0, len(input_list))]\n",
        "\n",
        "# def select_negative_vectors(negatives, key):\n",
        "#     \"\"\"\n",
        "#     Collects all negative vectors except for the one corresponding to the key.\n",
        "#     \"\"\"\n",
        "#     return [vec for k,v in negatives.items() if k != key for vec in v['known']]\n",
        "\n",
        "def get_random_triplet(sample, negatives):\n",
        "    \"\"\"\n",
        "    This function takes a sample and negatives, and returns a random triplet of anchor, positive, and negative.\n",
        "    \"\"\"\n",
        "    # Select the positive vector\n",
        "    positive = select_random_from_list(sample['positives'])\n",
        "\n",
        "    # Select the negative vector\n",
        "    negative = select_random_from_list(negatives[sample['key']])\n",
        "\n",
        "    return sample['anchor'], positive, negative\n",
        "\n",
        "def get_hard_triplet(sample, negatives, base_model, clf_model,):\n",
        "    \"\"\"\n",
        "    This function takes a sample, negatives, and a model, and returns a hard triplet of anchor, positive, and negative.\n",
        "    The sample with the lowest probability is the hardest positive sample,\n",
        "    while a high probability indicates that the model is confident in classifying it as positive.\n",
        "    Therefore, the lowest probability implies that the model has incorrectly classified it.\n",
        "    \"\"\"\n",
        "    anchor_rep = base_model.predict(np.array([sample['anchor']]), verbose=0)\n",
        "\n",
        "    ### ------ Positive ------ ###\n",
        "    # Compute distances between anchor and all positive vectors\n",
        "    positive_reps = base_model.predict(np.array(sample['positives']), verbose=0)\n",
        "    AP_reps = []\n",
        "    for rep in positive_reps:\n",
        "        comb = np.concatenate((anchor_rep[0], rep), axis=None)\n",
        "        AP_reps.append(comb)\n",
        "\n",
        "    # Select the hardest positive (the one with the lowest probability)\n",
        "    positive_distances = clf_model.predict(np.array(AP_reps), verbose=0)\n",
        "    hardest_positive = sample['positives'][np.argmin(positive_distances)]\n",
        "\n",
        "\n",
        "    ### ------ Negative ------ ###\n",
        "    # Collect all negative vectors and compute distances to anchor\n",
        "    negative_vectors = negatives[sample['key']]\n",
        "    negative_reps = base_model.predict(np.array(negative_vectors), verbose=0)\n",
        "    AN_reps = []\n",
        "    for rep in negative_reps:\n",
        "        comb = np.concatenate((anchor_rep[0], rep), axis=None)\n",
        "        AN_reps.append(comb)\n",
        "\n",
        "    # Select the hardest negative (the one with the highest probability)\n",
        "    negative_distances = clf_model.predict(np.array(AN_reps), verbose=0)\n",
        "    hardest_negative = negative_vectors[np.argmax(negative_distances)]\n",
        "\n",
        "    # # positive_distances = [compute_cosine_distance(pos_rep, anchor_rep[0]) for pos_rep in positive_reps]\n",
        "    # positive_distances = [np.sum(np.square(pos_rep - anchor_rep[0])) for pos_rep in positive_reps]\n",
        "\n",
        "    # # Select the hardest positive (the one with the largest distance)\n",
        "    # hardest_positive = sample['positives'][np.argmax(positive_distances)]\n",
        "\n",
        "    # # Collect all negative vectors and compute distances to anchor\n",
        "    # negative_vectors = select_negative_vectors(negatives, sample['key'])\n",
        "    # negative_reps = model.predict(np.array(negative_vectors), verbose=0)\n",
        "    # negative_distances = [np.sum(np.square(neg_rep - anchor_rep[0])) for neg_rep in negative_reps]\n",
        "\n",
        "    # # Select the hardest negative (the one with the smallest distance)\n",
        "    # hardest_negative = negative_vectors[np.argmin(negative_distances)]\n",
        "\n",
        "    return sample['anchor'], hardest_positive, hardest_negative\n",
        "\n",
        "def get_triplet(sample, negatives, base_model, clf_model, hard_triplet_probability):\n",
        "    \"\"\"\n",
        "    This function decides between selecting a hard triplet or a random triplet based on the hard_triplet_probability.\n",
        "    \"\"\"\n",
        "    if np.random.rand() < hard_triplet_probability:\n",
        "        # With a certain probability, choose the hardest triplet\n",
        "        return get_hard_triplet(sample, negatives, base_model, clf_model)\n",
        "    else:\n",
        "        # Otherwise, choose a random triplet\n",
        "        return get_random_triplet(sample, negatives)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLXLb3g8rOrA"
      },
      "source": [
        "#### Training on Semi-Hard Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJqrVYYqrsfn"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "patience = 10\n",
        "previous_loss = float('inf')\n",
        "\n",
        "hard_triplet_probability_start=0.5\n",
        "hard_triplet_probability_end=0.8\n",
        "\n",
        "early_stopping_2 = EarlyStopping(monitor='loss', patience=patience, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWfTVmvZrxav",
        "outputId": "c68edc37-245f-46e7-fec2-c9fd18b3b52f"
      },
      "outputs": [],
      "source": [
        "# Initial probability of selecting a hard triplet\n",
        "triplet_select_probability = hard_triplet_probability_start\n",
        "\n",
        "# Iterate over each epoch\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  # Initialize empty lists for anchor, positive, negative samples and labels\n",
        "  anchor_samples = []\n",
        "  positive_samples = []\n",
        "  negative_samples = []\n",
        "  labels = []\n",
        "\n",
        "  # Iterate over triplet samples\n",
        "  for _, sample in triplet_sample_candidates.items():\n",
        "    # Get the anchor, positive, negative samples\n",
        "    anchor, positive, negative = get_triplet(sample, negative_vectors_dict, base_network, clf_network, triplet_select_probability)\n",
        "    # Add samples to their respective lists\n",
        "    anchor_samples.append(anchor)\n",
        "    positive_samples.append(positive)\n",
        "    negative_samples.append(negative)\n",
        "    labels.append(0)\n",
        "\n",
        "  # Convert lists to numpy arrays\n",
        "  anchor_samples = np.array(anchor_samples)\n",
        "  positive_samples = np.array(positive_samples)\n",
        "  negative_samples = np.array(negative_samples)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  # Train the model on current epoch's data\n",
        "  siamese_model.fit([anchor_samples, positive_samples, negative_samples], labels,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stopping_2, cp_save])\n",
        "\n",
        "  # Gradually increase the probability of choosing a hard triplet\n",
        "  triplet_select_probability += (hard_triplet_probability_end - hard_triplet_probability_start) / num_epochs\n",
        "\n",
        "  # Uncomment the following section for Early Stopping\n",
        "  # Check if current epoch is a 'patience' epoch\n",
        "  if epoch % patience == 0 and epoch != 0:\n",
        "    current_loss = siamese_model.history.history['loss'][-1]\n",
        "    # Check if loss is increasing or constant, if yes, then stop training\n",
        "    if current_loss >= previous_loss:\n",
        "      print(\"Early stopping triggered. Stopping training.\")\n",
        "      break\n",
        "    else:\n",
        "      # Update previous loss with current loss\n",
        "      previous_loss = current_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9k4beR9tu4o"
      },
      "source": [
        "# Inference and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdy5TBdrtxTV"
      },
      "outputs": [],
      "source": [
        "def generate_concatenated_vectors(data, base_network):\n",
        "  concatenated_vectors = []\n",
        "  labels = []\n",
        "\n",
        "  for k, v in tqdm(data.items(), total=len(data)):\n",
        "    # Process known vectors\n",
        "    known_feature_vectors = base_network.predict(np.array(v['known']), verbose=0)\n",
        "\n",
        "    # Process unknown vectors\n",
        "    unknown_feature_vectors = base_network.predict(np.array(v['unknown']), verbose=0)\n",
        "\n",
        "    # Compute the average feature vector\n",
        "    author_representation = np.mean(known_feature_vectors, axis=0)\n",
        "    unknown_representation = np.mean(unknown_feature_vectors, axis=0)\n",
        "\n",
        "    concate_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n",
        "\n",
        "    concatenated_vectors.append(concate_vec)\n",
        "    labels.append(v['label'])\n",
        "\n",
        "  return np.array(concatenated_vectors), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AxJfKnxuRHm",
        "outputId": "9b41968e-61f9-4dc6-9d62-ee0e62dbbee0"
      },
      "outputs": [],
      "source": [
        "# Build train siamese_embedding dataset\n",
        "train_siamese_vec, train_siamese_labels = generate_concatenated_vectors(train_data, base_network)\n",
        "\n",
        "# Build val siamese_embedding dataset\n",
        "val_siamese_vec, val_siamese_labels = generate_concatenated_vectors(val_data, base_network)\n",
        "\n",
        "# Build test siamese_embedding dataset\n",
        "test_siamese_vec, test_siamese_labels = generate_concatenated_vectors(test_data, base_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYLuWkM_QbYR"
      },
      "outputs": [],
      "source": [
        "clf_network.compile(optimizer='adam',\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy', tf.keras.metrics.AUC()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53b-IY7I2xh-",
        "outputId": "b70346fe-f58b-447b-9e73-6b04053a4741"
      },
      "outputs": [],
      "source": [
        "clf_early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
        "clf_history = clf_network.fit(train_siamese_vec, train_siamese_labels,\n",
        "                              epochs=1000,\n",
        "                              verbose=1,\n",
        "                              validation_data = (val_siamese_vec, val_siamese_labels),\n",
        "                              callbacks=[clf_early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sehqxBD07HCR",
        "outputId": "69ed0d1c-d74e-469f-c032-9c43f5522919"
      },
      "outputs": [],
      "source": [
        "res = clf_network.evaluate(test_siamese_vec, test_siamese_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40B63860CvLq",
        "outputId": "dba08f08-a775-446f-a493-d4c0ca49c4f8"
      },
      "outputs": [],
      "source": [
        "res = clf_network.evaluate(test_siamese_vec, test_siamese_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C89dzA6bxVsf"
      },
      "source": [
        "# Calculate Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRskuFpyxYFx"
      },
      "outputs": [],
      "source": [
        "def calculate_score(y_predict, y_true):\n",
        "    n = len(y_predict)\n",
        "    n_correct = 0\n",
        "    n_unknown = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        if y_predict[i] > 0.5:\n",
        "            prediction = 1\n",
        "        elif y_predict[i] < 0.5:\n",
        "            prediction = 0\n",
        "        else:\n",
        "            n_unknown += 1\n",
        "            continue\n",
        "\n",
        "        if prediction == y_true[i]:\n",
        "            n_correct += 1\n",
        "\n",
        "    c_1 = (n_correct + (n_unknown * n_correct / n)) / n\n",
        "    auc = tf.keras.metrics.AUC()(y_true, y_predict)\n",
        "    score = auc.numpy() * c_1\n",
        "\n",
        "    return c_1, auc.numpy(), score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdsN95EnxZhj",
        "outputId": "7d9d4627-f80f-4af7-e564-81860a51ac1f"
      },
      "outputs": [],
      "source": [
        "nn_pred = clf_network.predict(test_siamese_vec)\n",
        "c_1, auc, score = calculate_score(nn_pred, test_siamese_labels)\n",
        "\n",
        "print(\"C@1:\", round(c_1, 3))\n",
        "print(\"AUC:\", round(auc, 3))\n",
        "print(\"Final Score:\", round(score, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVadtDLybBiq"
      },
      "source": [
        "# Production Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1erptilSa5wM"
      },
      "source": [
        "### Saving/Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmdVAnu2fTCL"
      },
      "outputs": [],
      "source": [
        "# Save the generated Word2Vec model\n",
        "default_word2vec_save = \"word2vec.model\"\n",
        "word2vec_model.save(default_word2vec_save)\n",
        "\n",
        "# Save weights for the generated SiameseNet model\n",
        "default_checkpoints_file = \"model_weights/cp.ckpt\"\n",
        "siamese_model.save_weights(default_checkpoints_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2D04KHSbFk9"
      },
      "outputs": [],
      "source": [
        "# Load a saved Word2Vec Model\n",
        "default_word2vec_save = \"word2vec.model\"\n",
        "loadW2v = lambda path : gensim.models.Word2Vec.load(path)\n",
        "\n",
        "# Rebuild a SiameseNet model from saved weights\n",
        "def buildSiameseNet(checkpoint_file: str, embedding_dim: tuple = (323,)) -> SiameseNet:\n",
        "\t\"\"\"Construct the SiameseNet model using code from PAN14_data_demo.ipynb\n",
        "\t\tusing saved weights at checkpoint_dir\n",
        "\t\tembedding_dim defines the input shape for the model, the default from the build process is (323,None)\"\"\"\n",
        "\n",
        "\t# Create sub-model frame\n",
        "\tbase_network = create_base_network(embedding_dim)\n",
        "\tclf_network = create_clf_network(base_network.output_shape[1]*2)\n",
        "\n",
        "\t# Create main model frame\n",
        "\tsiamese_model = SiameseNet(base_network, clf_network)\n",
        "\t\n",
        "\t# Compile models (not necessary if it doesn't need to be trained further)\n",
        "\t#siamese_model.compile(optimizer='adam', loss=customer_loss)\n",
        "\t#clf_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
        "\n",
        "\tsiamese_model.load_weights(checkpoint_file).expect_partial()\n",
        "\t\n",
        "\treturn siamese_model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mujo54RggYRc"
      },
      "source": [
        "### Proof-of-concept\n",
        "General loading and testing functions to demonstrate the model running over singlar profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Utility functions, not directly related to the model but used alot later ###\n",
        "def unwrap(var):\n",
        "\t\"\"\"Function to extract variables nested inside 1-element lists/arrays\"\"\"\n",
        "\tis_array = lambda var : isinstance(var, (list, tuple, set, np.ndarray))\n",
        "\twhile is_array(var) and len(var) == 1: var = var[0]\n",
        "\treturn var\n",
        "\n",
        "def choose_random_folders(path, num):\n",
        "\t\t\"\"\"Return a list of random folders from a given root\"\"\"\n",
        "\t\tdirs = [ os.path.join(path, x) for x in os.listdir(path) if os.path.isdir(os.path.join(path, x)) ]\n",
        "\t\treturn random.choices(dirs, k=num)\n",
        "\n",
        "print(f\"Floats will need to be within {sys.float_info.epsilon} to be considered equal!\")\n",
        "def float_cmp(f1, f2, epsilon=sys.float_info.epsilon) -> tuple[bool,float]:\n",
        "\t\"\"\"Basic float compare, True is values are approx. equal. Also return the difference regardless\"\"\"\n",
        "\tdiff = abs(f1 - f2)\n",
        "\treturn diff <= epsilon, diff\n",
        "\n",
        "def dataset_compare(setA:list, setB:list, verbose = False) -> None:\n",
        "\t\"\"\"Compare all elements in two lists of flaots and returns a similarity score + max deviation\"\"\"\n",
        "\n",
        "\tif len(setA) != len(setB): print(\"WARNING: arrays are different lengths!\")\n",
        "\tmin_len = min(len(setA), len(setB))\n",
        "\n",
        "\tmatches = 0\n",
        "\tmax_diff = 0.0\n",
        "\tfor i in range(min_len):\n",
        "\t\tres, diff = float_cmp(setA[i], setB[i])\n",
        "\t\tif diff > max_diff: max_diff = diff\n",
        "\n",
        "\t\tif res:\n",
        "\t\t\tmatches += 1\n",
        "\t\telif verbose:\n",
        "\t\t\t# Optionally print every single value that doesn't pass\n",
        "\t\t\tprint(f\"{setA[i]} != {setB[i]} (diff = {diff})\")\n",
        "\t\n",
        "\tscore = matches / min_len\n",
        "\tprint(f\"{score:.1%} pass, with max devation of {max_diff:.9%}\")\n",
        "\n",
        "def deep_clean(path, inc_root = False):\n",
        "\t\"\"\"Clean up (remove) the contents of a folder and optionally the folder itself\"\"\"\n",
        "\n",
        "\tif not os.path.isdir(path):\n",
        "\t\tprint(f\"Invalid path: {path}\")\n",
        "\t\treturn\n",
        "\n",
        "\tif len(os.listdir(path)):\n",
        "\t\tfor root, dirs, files in os.walk(path, topdown=False):\n",
        "\t\t\tfor file in files:\n",
        "\t\t\t\tos.remove(os.path.join(root, file))\n",
        "\t\t\tfor dir in dirs:\n",
        "\t\t\t\tos.rmdir(os.path.join(root, dir))\n",
        "\telse:\n",
        "\t\tprint(f\"{path} is already empty!\")\n",
        "\n",
        "\tif inc_root:\n",
        "\t\tos.rmdir(path)\n",
        "\t\tprint(f\"Removed {path} and contents\")\n",
        "\telse:\n",
        "\t\tprint(f\"Removed contents of {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOoL-jotg4I1"
      },
      "outputs": [],
      "source": [
        "def load_from_dir(path) -> dict:\n",
        "\t\"\"\"Load raw known/unknown text data from a folder\"\"\"\n",
        "\tfiles = []\n",
        "\tfor file in os.listdir(path):\n",
        "\t\tfiles.append(os.path.join(path, file))\n",
        "\t\n",
        "\ttexts = {}\n",
        "\ttexts['known'], texts['unknown'] = extract_text_from_files(files)\n",
        "\n",
        "\treturn texts\n",
        "\n",
        "def vectorize_single(known,unknown,w2v_model):\n",
        "\t\"\"\"Converts a single set of texts (instead of the entire corpus) to vectors\"\"\"\n",
        "\tvectors = {\n",
        "\t\t'known': get_vectors(known, w2v_model),\n",
        "\t\t'unknown': get_vectors(unknown, w2v_model)\n",
        "\t\t# 'label' not included since it's not relevant outside of training\n",
        "\t}\n",
        "\treturn vectors\n",
        "\n",
        "def concatenate_vectors_single(vectors, base_network):\n",
        "\tknown_feature_vectors = base_network.predict(np.array(vectors['known']), verbose=0)\n",
        "\tunknown_feature_vectors = base_network.predict(np.array(vectors['unknown']), verbose=0)\n",
        "\n",
        "\tauthor_representation = np.mean(known_feature_vectors, axis=0)\n",
        "\tunknown_representation = np.mean(unknown_feature_vectors, axis=0)\n",
        "\n",
        "\tconcate_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n",
        "\treturn concate_vec\n",
        "\n",
        "def predict_once(texts, w2v = word2vec_model, base = base_network, clf = clf_network) -> float:\n",
        "\t\"\"\"Run the model to make a single prediction\"\"\"\n",
        "\tknown_data = texts['known']\n",
        "\tunknown_data = texts['unknown']\n",
        "\t\n",
        "\t# Early return if there is no unknown text included\n",
        "\tif len(unknown_data) == 0:\n",
        "\t\treturn 0.0\n",
        "\n",
        "\tvectors = vectorize_single(known_data, unknown_data, w2v)\n",
        "\tconcats = concatenate_vectors_single(vectors, base)\n",
        "\n",
        "\tprediction = clf.predict(np.expand_dims(concats, axis=0), verbose=0)\n",
        "\n",
        "\t# Convert the Tensor to a numpy array and flatten, as output shape will be (1,1)\n",
        "\treturn unwrap(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayf2N9dLnB4p"
      },
      "outputs": [],
      "source": [
        "### Run 'predict_once' using both the live and saved/loaded model on random data to compare results ###\n",
        "print(f\"{'Loaded':>20} vs {'Generated':<20} (Pass)  Source\")\n",
        "\n",
        "def load_and_test(path):\n",
        "  loaded_word2vec_model = loadW2v(default_word2vec_save)\n",
        "  loaded_siamese_model = buildSiameseNet(default_checkpoints_file)\n",
        "\n",
        "  # Using the base_network and clf_network variables from the generation\n",
        "  loaded = predict_once(load_from_dir(path), loaded_word2vec_model, loaded_siamese_model.base, loaded_siamese_model.clf)\n",
        "  generated = predict_once(load_from_dir(path), word2vec_model, base_network, clf_network)\n",
        "  print(f\"{loaded:>20} vs {generated:<20} ({loaded == generated})  {path}\")\n",
        "\n",
        "# If the saving/loading process works then values should ideally be *exactly* the same\n",
        "for dir in choose_random_folders(\"./data/test_data\", 10):\n",
        "  load_and_test(dir)\n",
        "\n",
        "for dir in choose_random_folders(\"./data/train_data\", 10):\n",
        "  load_and_test(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Test the validity of 'predict_once' by comparing to\n",
        "# the generated batch prediction results (from earlier) ###\n",
        " \n",
        "def test_predict_function(corpus, concats, name=None):\n",
        "  generated = [ unwrap(x) for x in clf_network.predict(concats, verbose=0) ]\n",
        "  loaded = [ predict_once(text) for text in tqdm(corpus.values(), desc=name) ]\n",
        "  \n",
        "  dataset_compare(generated, loaded)\n",
        "\n",
        "# Again values *should* be identical\n",
        "test_predict_function(test_corpus, test_siamese_vec, \"Test Data\")\n",
        "test_predict_function(val_corpus, val_siamese_vec, \"Val Data\")\n",
        "#test_predict_function(train_corpus, train_siamese_vec, \"Train Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test against the entire dataset\n",
        "def test_against_dataset(dataset, concats, name = None, verbose = False):\n",
        "    loaded_siamese_model = buildSiameseNet(default_checkpoints_file)\n",
        "    loaded_word2vec_model = loadW2v(default_word2vec_save)\n",
        "\n",
        "    generated = [ unwrap(x) for x in clf_network.predict(concats, verbose=0) ]\n",
        "\n",
        "    loaded = []\n",
        "    keys = []\n",
        "    for key, text in tqdm(dataset.items(), desc=name):\n",
        "      keys.append(key)\n",
        "\n",
        "      try:\n",
        "        known_data = text['known']\n",
        "        unknown_data = text['unknown']\n",
        "\n",
        "        vectors = vectorize_single(known_data, unknown_data, loaded_word2vec_model)\n",
        "        concats = concatenate_vectors_single(vectors, loaded_siamese_model.base)\n",
        "\n",
        "        prediction = unwrap(loaded_siamese_model.clf(np.array([concats])).numpy())\n",
        "        loaded.append(prediction)\n",
        "      except ValueError:\n",
        "        loaded.append(0.0)\n",
        "\n",
        "    dataset_compare(generated, loaded)\n",
        "\n",
        "test_against_dataset(test_corpus, test_siamese_vec, \"Test Data\")\n",
        "test_against_dataset(val_corpus, val_siamese_vec, \"Val Data\")\n",
        "#test_against_dataset(train_corpus, train_siamese_vec, \"Train Dataset\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WiNv3hmluyJt"
      },
      "source": [
        "### Self-Contained Classes\n",
        "Handles both loading and running the 3 models within a single, self-contained class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Additional utility functions for the class ###\n",
        "def strip_text(data):\n",
        "\tif type(data) is str: data = data.splitlines()\n",
        "\n",
        "\ttext = []\n",
        "\tfor line in data:\n",
        "\t\tif type(line) is not str: line = str(line)\n",
        "\t\tcleaned = line.strip().lstrip(\"\\ufeff\")\n",
        "\t\ttext.append(cleaned)\n",
        "\n",
        "\treturn text\n",
        "\n",
        "def setupNltk(path = f\"{os.curdir}/nltk_data\") -> None:\n",
        "\t\"\"\"Set up the NLTK path and downloads datapacks (if required)\"\"\"\n",
        "\tnltk.data.path = [ path ]\n",
        "\tnltk.download([\"punkt\", \"stopwords\",\"wordnet\"], download_dir=nltk.data.path[0], quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dHkb5wIuwFS"
      },
      "outputs": [],
      "source": [
        "class StyloNet:\n",
        "\t\"\"\"Contains the whole stylometry model, functions score, score_multi, predict and predict_multi\n",
        "\t\tcan be called to run the stylometry model on a text.\n",
        "\n",
        "\t\tInput format for predictions is the following:\n",
        "\t\t\ttexts = {\n",
        "\t\t\t\t'known': list[str] (known text(s))\n",
        "\t\t\t\t'unknown': list[str] (unknown text(s))\n",
        "\t\t\t}\n",
        "\t\tAll lists for texts can be multi-dimensional, as long as it only ends in strings\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, profile = None, profile_dir:str = \"stylometry_models\"):\n",
        "\t\t# Load the profile path (if no profile is specified, use the current directory)\n",
        "\t\tprofile_path = os.path.join(profile_dir, profile) if profile else os.curdir\n",
        "\n",
        "\t\t# Try loading the manifest, else use default values\n",
        "\t\ttry:\n",
        "\t\t\twith open(os.path.join(profile_path, \"manifest.json\"), \"rb\") as f:\n",
        "\t\t\t\tmanifest = json.load(f)\n",
        "\t\texcept FileNotFoundError:\n",
        "\t\t\t# If the manifest isn't there, set to an empty dictionary to use default values\n",
        "\t\t\tmanifest = {}\n",
        "\n",
        "\t\tself.valid_threshold = manifest.get(\"valid_threshold\", 0.5)\n",
        "\t\tembedding_dim = manifest.get(\"embedding_dim\", (323,))\n",
        "\t\tself.vector_length = manifest.get(\"word_vector_size\", 300)\n",
        "\n",
        "\t\tnltk_path = os.path.join(profile_path, manifest.get(\"nltk_data\", \"nltk_data\"))\n",
        "\t\tw2v_save = os.path.join(profile_path, manifest.get(\"word2vec\", \"word2vec.model\"))\n",
        "\t\tmodel_checkpoints = os.path.join(profile_path, manifest.get(\"ckpts\", \"model_weights/cp.ckpt\"))\n",
        "\n",
        "\t\tif not os.path.isdir(nltk_path) or len(os.listdir(nltk_path)) <= 0:\n",
        "\t\t\t# If the nltk path isn't populated, assume it probably needs to be downloaded\n",
        "\t\t\tsetupNltk(nltk_path)\n",
        "\n",
        "\t\t# Load Word2Vec model\n",
        "\t\tself.word2vec = loadW2v(w2v_save)\n",
        "\n",
        "\t\t# Redefine and load the SiameseNet model from checkpoints\n",
        "\t\tself.siamese_model = buildSiameseNet(model_checkpoints, embedding_dim)\n",
        "\t\tself.base_network = self.siamese_model.base\n",
        "\t\tself.clf_network = self.siamese_model.clf\n",
        "\n",
        "\tdef _vectorize(self,text : dict):\n",
        "\t\tvectors = {\n",
        "\t\t\t'known': get_vectors(text['known'], self.word2vec, self.vector_length),\n",
        "\t\t\t'unknown': get_vectors(text['unknown'], self.word2vec, self.vector_length)\n",
        "\t\t}\n",
        "\t\treturn vectors\n",
        "\n",
        "\tdef _vectorize_multi(self, texts: list[dict]):\n",
        "\t\tvectors = []\n",
        "\t\tfor text in texts:\n",
        "\t\t\tvectors.append(self._vectorize(text))\n",
        "\t\treturn vectors\n",
        "\n",
        "\tdef _concatenate(self, vectors: dict):\n",
        "\t\tknown_feature_vectors = self.base_network.predict(np.array(vectors['known']), verbose=0)\n",
        "\t\tunknown_feature_vectors = self.base_network.predict(np.array(vectors['unknown']), verbose=0)\n",
        "\n",
        "\t\tauthor_representation = np.mean(known_feature_vectors, axis=0)\n",
        "\t\tunknown_representation = np.mean(unknown_feature_vectors, axis=0)\n",
        "\n",
        "\t\tconcat_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n",
        "\t\treturn concat_vec\n",
        "\n",
        "\tdef _concatenate_multi(self, vectors : list[dict]):\n",
        "\t\tconcats = []\n",
        "\t\tfor vec in vectors:\n",
        "\t\t\tconcats.append(self._concatenate(vec))\n",
        "\t\treturn np.array(concats)\n",
        "\n",
        "\t### Interface Functions ###\n",
        "\tdef score(self, texts : dict) -> float:\n",
        "\t\t\"\"\"Run the model and return the similarity score as a decimal\"\"\"\n",
        "\t\tif len(texts['unknown']) == 0: return 0  # Incase of empty unknown set\n",
        "\n",
        "\t\tvectors = self._vectorize(texts)\n",
        "\t\tconcats = self._concatenate(vectors)\n",
        "\n",
        "\t\tprediction = self.clf_network.predict(np.expand_dims(concats, axis=0), verbose=0)\n",
        "\n",
        "\t\t# Convert to numpy array and flatten, as output shape will be (1,1)\n",
        "\t\treturn unwrap(prediction)\n",
        "\n",
        "\tdef score_batch(self, texts: list|dict) -> list[float]|dict:\n",
        "\t\t\"\"\"Calculate score over a list or dictionary of texts and return a list/dict of the results\"\"\"\n",
        "\n",
        "\t\t# Return a key/value pair generator for both a list and existing dictionary\n",
        "\t\tunpack = lambda texts: texts.items() if type(texts) is dict else enumerate(data)\n",
        "\n",
        "\t\t# Check whether a value is valid input (i.e. it's unknown text is non-empty)\n",
        "\t\tgood_input = lambda text: type(text['unknown']) is list and len(text['unknown']) != 0\n",
        "\n",
        "\t\t# Convert input to a dictionary, if not already.\n",
        "\t\t# Ensures bad input can be filtered out and given a 0.0 rating at the end\n",
        "\t\tdata = { k: v for k, v in unpack(texts) if good_input(v) }\n",
        "\t\t# Important for list ordering and including all keys in dictionaries\n",
        "\n",
        "\t\t# Run the predictions\n",
        "\t\tvectors = self._vectorize_multi(data.values())\n",
        "\t\tconcates = self._concatenate_multi(vectors)\n",
        "\t\tpredictions = { k: unwrap(v) for k, v in zip(data.keys(), self.clf_network.predict(concates, verbose=0)) }\n",
        "\n",
        "\t\t# Format the output (unwrapping already done above)\n",
        "\t\tif type(texts) is list:\n",
        "\t\t\tresults = [ predictions.get(x, 0.0) for x in range(len(texts)) ]\n",
        "\t\telse:\n",
        "\t\t\tresults = { k: predictions.get(k, 0.0) for k in texts.keys() }\n",
        "\n",
        "\t\treturn results\n",
        "\n",
        "\tdef predict(self, texts: dict) -> bool:\n",
        "\t\t\"\"\"Calculate score and return a prediction based on the predetermined threshold, returns a boolean result\"\"\"\n",
        "\t\treturn self.score(texts) >= self.valid_threshold\n",
        "\n",
        "\tdef predict_batch(self, texts: list|dict) -> list[bool]|dict:\n",
        "\t\t\"\"\"Run predict over a list/dict of texts and return the result with a boolean result\"\"\"\n",
        "\t\tscores = self.score_batch(texts)\n",
        "\t\tpred = lambda s : s >= self.valid_threshold\n",
        "\n",
        "\t\tif type(texts) is dict:\n",
        "\t\t\tresults = {}\n",
        "\t\t\tfor key, val in scores.items():\n",
        "\t\t\t\tresults[key] = pred(val)\n",
        "\t\telse:\n",
        "\t\t\tresults = []\n",
        "\t\t\tfor val in scores:\n",
        "\t\t\t\tresults.append(pred(val))\n",
        "\n",
        "\t\treturn results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TgkfIYdVgrY3"
      },
      "source": [
        "### Testing and Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: tests on train_corpus are commented out for now, as the *train_siamese_vectors* and *train_corpus* data are different sizes. Puts the data out of phase and makes the entire comparison worthless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "### Test the StyloNet class, defined above, to ensure it generates the same output on all datasets ###\n",
        "concat_predict = lambda concats, model: [ unwrap(x) for x in model.predict(concats) ]\n",
        "# Get results from the generated model\n",
        "generated_test_results = concat_predict(test_siamese_vec, clf_network)\n",
        "#generated_train_results = concat_predict(train_siamese_vec, clf_network)\n",
        "generated_val_results = concat_predict(val_siamese_vec, clf_network)\n",
        "\n",
        "# Run predictions over the same dataset using the loaded model\n",
        "loaded_model = StyloNet()\n",
        "loaded_test_results = loaded_model.score_batch(test_corpus)\n",
        "#loaded_train_results = loaded_model.score_batch(train_corpus)\n",
        "loaded_val_results = loaded_model.score_batch(val_corpus)\n",
        "\n",
        "# Verify results\n",
        "dataset_compare(generated_test_results, list(loaded_test_results.values()))\n",
        "#dataset_compare(generated_train_results, list(loaded_train_results.values())\n",
        "dataset_compare(generated_val_results, list(loaded_val_results.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Verify 'score' and 'score_batch' return the same results ###\n",
        "def test_function_accuracy(corpus, name=None):\n",
        "  loaded_model = StyloNet()\n",
        "\n",
        "  batch_res = list(loaded_model.score_batch(corpus).values())\n",
        "\n",
        "  individual_res = []\n",
        "  for text in tqdm(corpus.values(), desc=name):\n",
        "    individual_res.append(loaded_model.score(text))\n",
        "\n",
        "  dataset_compare(batch_res, individual_res)\n",
        "\n",
        "test_function_accuracy(test_corpus, \"Test Data\")\n",
        "test_function_accuracy(val_corpus, \"Val Data\")\n",
        "test_function_accuracy(train_corpus, \"Train Data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: Results from the test cases above show that the freshly-generated and loaded model have ALMOST identical output.\n",
        "\n",
        "The variation in output seen only seems to occur when comparing prediction run as a batch over an entire dataset to running the prediction individually on each profile.\n",
        "\n",
        "As the maximum discrepancy between between results is < 0.00001 in basically all cases, this is more than enough for the result to be considered the same in virtually every application.\n",
        "\n",
        "Saving and loading the model doesn't change the analysis results at all (see 2 cells above) however, it's *only* running as a batch vs individually that affects results."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profile Handling\n",
        "Code to save/load/remove profiles from the model. The StyloNet class above also contains the required code, for easy export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_profile(name, profile_dir = \"models\") -> None:\n",
        "  # Ensure destination exists\n",
        "  if not os.path.isdir(profile_dir):\n",
        "    os.mkdir(profile_dir)\n",
        "    print(f\"Created profile base directory: {profile_dir}\")\n",
        "\n",
        "  try:\n",
        "    manifest = {}\n",
        "    manifest['name'] = name\n",
        "    abs_path = os.path.join(profile_dir, name)\n",
        "    os.mkdir(abs_path)\n",
        "    print(f\"Created profile dir: {abs_path}\")\n",
        "    \n",
        "    # save embedding dim\n",
        "    manifest['embedding_dim'] = unwrap(embedding_dim)\n",
        "    print(f\"Saved embedding_dim (unwrapped): {manifest['embedding_dim']}\")\n",
        "\n",
        "    # save siamese net model weights\n",
        "    model_save = \"model_weights/cp.ckpt\"\n",
        "    abs_path = os.path.join(profile_dir, name, model_save)\n",
        "    manifest['ckpts'] = model_save\n",
        "    os.mkdir(os.path.dirname(abs_path))\n",
        "    siamese_model.save_weights(abs_path)\n",
        "    print(f\"Saved siamese_net weights: {abs_path}\")\n",
        "    \n",
        "    # save word vector size\n",
        "    manifest['word_vector_size'] = w2v_vector_size\n",
        "    print(f\"Saved Word Vector Size: {manifest['word_vector_size']}\")\n",
        "\n",
        "    # save word2vec model weights\n",
        "    word2vec_save = \"word2vec.model\"\n",
        "    abs_path = os.path.join(profile_dir, name, word2vec_save)\n",
        "    manifest['word2vec'] = word2vec_save\n",
        "    word2vec_model.save(abs_path)\n",
        "    print(f\"Saved word2vec model: {abs_path}\")\n",
        "\n",
        "    # save pre-downloaded ntlk data\n",
        "    nltk_data_save = \"nltk_data\"\n",
        "    abs_path = os.path.join(profile_dir, name, nltk_data_save)\n",
        "    manifest['nltk_data'] = nltk_data_save\n",
        "    nltk.download([\"punkt\", \"stopwords\",\"wordnet\"], download_dir=abs_path)\n",
        "    print(f\"Saved nltk data: {abs_path}\")\n",
        "\n",
        "    manifest['valid_threshold'] = 0.5\n",
        "    print(f\"Saved validity threshold: {manifest['valid_threshold']}\")\n",
        "\n",
        "    # save the manifest\n",
        "    manifest_save = \"manifest.json\"\n",
        "    abs_path = os.path.join(profile_dir, name, manifest_save)\n",
        "    with open(abs_path, \"w\") as f:\n",
        "      f.write(json.dumps(manifest, indent=4))\n",
        "    print(f\"Saved manifest: {abs_path}\")\n",
        "      \n",
        "  except OSError:\n",
        "    # some file IO failed, do a bit of cleanup then continue the error\n",
        "    deep_clean(os.path.join(profile_dir, name), True)\n",
        "    raise OSError(\"Profile failed to save!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_profile(name, profile_dir=\"models\") -> dict:\n",
        "    abs_path = os.path.join(profile_dir, name)\n",
        "    loads = {}\n",
        "    \n",
        "    with open(os.path.join(abs_path, \"manifest.json\"), \"r\") as f:\n",
        "        manifest = json.load(f)\n",
        "    \n",
        "    # Load word2vec model\n",
        "    word2vec_save = os.path.join(abs_path, manifest['word2vec'])\n",
        "    word2vec = loadW2v(word2vec_save)\n",
        "    loads['word2vec'] = word2vec\n",
        "    loads['word_vector_size'] = manifest['word_vector_size']\n",
        "\n",
        "    # Load siamese netdef remove_profile(name, profile_dir=\"models\"):\n",
        "    embedding_dim = (manifest['embedding_dim'],) if type(manifest['embedding_dim']) is int else tuple(manifest['embedding_dim'])\n",
        "    loads['embedding_dim'] = embedding_dim\n",
        "    loads['siamese_net'] = buildSiameseNet(os.path.join(abs_path, manifest['ckpts']), embedding_dim)\n",
        "\n",
        "    loads['nltk_data'] = manifest['nltk_data']\n",
        "    loads['valid_threshold'] = manifest['valid_threshold']\n",
        "\n",
        "    return loads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_profile(name, profile_dir=\"models\"):\n",
        "    deep_clean(os.path.join(profile_dir, name), True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profile Management and Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save a profile with a custom name\n",
        "print(\"Input profile name to save the weights (empty to skip saving)\")\n",
        "name = input(\"profile name? \")\n",
        "if len(name) > 0:\n",
        "\tsave_profile(name.lower().strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test loading a model\n",
        "print(\"Input a profile name to test if it loads correctly (empty to skip)\")\n",
        "name = input(\"profile name? \")\n",
        "if len(name) > 0:\n",
        "    load_test = load_profile(name)\n",
        "    print(f\"Loaded SiameseNet? {type(load_test['siamese_net']) is SiameseNet}\")\n",
        "    print(f\"Loaded Word2Vec? {type(load_test['word2vec']) is gensim.models.Word2Vec}\")\n",
        "    print(f\"Embedding Dim? {load_test['embedding_dim']}\")\n",
        "    print(f\"Word Vector Size? {load_test['word_vector_size']}\")\n",
        "    print(f\"Validity Threshold? {load_test['valid_threshold']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"WARNING: This will remove all SiameseNet weights, Word2Vec saved models and downloaded nltk_data!\")\n",
        "if input(\"Are you sure? \").lower().strip() == \"yes\":\n",
        "  # nltk downloads, if they're in the working directory\n",
        "  deep_clean(\"nltk_data\", True)\n",
        "\n",
        "  # checkpoint information\n",
        "  deep_clean(\"model_weights\", False)\n",
        "\n",
        "  # saved Word2Vec model, if it exists\n",
        "  try:\n",
        "    os.remove(default_word2vec_save)\n",
        "    print(\"Removed Word2Vec.model\")\n",
        "  except FileNotFoundError:\n",
        "    print(\"Invalid file: Word2Vec.model\")\n",
        "\n",
        "else:\n",
        "  print(\"Abort\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RkBeR4eOPEm2",
        "uFDDT_rmRokt",
        "THrYBCxhimox",
        "mmK7NtP31jdn",
        "FC2AmF8om5V4",
        "M9k4beR9tu4o",
        "YkYSjARCxBZ0",
        "C89dzA6bxVsf"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
