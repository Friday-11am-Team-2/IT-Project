{"cells":[{"cell_type":"markdown","metadata":{"id":"Xt4R6YMSF7sG"},"source":["# Set Up\n","Set up the Google Colab environment and import dependent libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6691,"status":"ok","timestamp":1691915575746,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"9rlYEd4wF2tU","outputId":"2075941d-d94c-4fe5-ba6c-d97a6136ee39"},"outputs":[],"source":["#Loading data from Google drive\n","import os\n","try:\n","\tfrom google.colab import drive\n","\tdrive.mount('/content/drive')\n","\tos.chdir(\"/content/drive/My Drive/Research_Proj/Project_Code/PAN14_Code\")\n","except ImportError:\n","\tprint(\"Google Colab isn't real, it can't hurt you.\")\n","\tpass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12072,"status":"ok","timestamp":1691915587815,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"Pj0ew-WfF-Fv","outputId":"09dad22f-8edc-4ead-e192-4bcb1ef8ff7d"},"outputs":[],"source":["import json\n","import math\n","import csv\n","import numpy as np\n","import glob\n","import pickle\n","import itertools\n","from collections import Counter\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers.schedules import PolynomialDecay\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.test.utils import common_texts\n","from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.cluster import KMeans\n","from scipy.spatial.distance import cosine\n","from sklearn.svm import SVC\n","from sklearn.model_selection import KFold\n","from collections import defaultdict\n","\n","import networkx as nx\n","import random\n","from tqdm import tqdm\n","from urllib.request import urlretrieve\n","import matplotlib.pyplot as plt\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bryce - separated from imports and changed data directory so it doesn't make a mess\n","nltk.data.path = [ f\"{os.getcwd()}/nltk_data\" ]\n","nltk.download([\"punkt\", \"stopwords\",\"wordnet\"], download_dir=nltk.data.path[0])"]},{"cell_type":"markdown","metadata":{"id":"Mepv72EIF_kq"},"source":["# Data Process\n","Read the data in and save it in the dict."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiwPGxTzI5Gl"},"outputs":[],"source":["def get_data_directory_path(subdirectory):\n","    return os.path.join('data', subdirectory)\n","\n","\n","def get_json_file_path(data_directory, file_name):\n","    return os.path.join(data_directory, file_name)\n","\n","\n","def read_json_file(file_path):\n","    with open(file_path) as file:\n","        data = json.load(file)\n","    return data\n","\n","\n","def extract_text_from_files(file_paths):\n","    known_text, unknown_text = [], []\n","\n","    for file_path in file_paths:\n","        text_lines = []\n","        with open(file_path, 'r') as file:\n","            for line in file:\n","                cleaned_line = line.strip().lstrip(\"\\ufeff\")\n","                text_lines.append(cleaned_line)\n","        if 'unknown' in file_path:\n","            unknown_text.append(text_lines)\n","        else:\n","            known_text.append(text_lines)\n","\n","    return known_text, unknown_text\n","\n","\n","def build_corpus(data_directory, content_data, label_data):\n","    corpus = {}\n","\n","    for index in tqdm(range(len(content_data['problems']))):\n","        problem_file_paths = glob.glob(os.path.join(data_directory, content_data['problems'][index], '*'))\n","\n","        if not problem_file_paths:\n","            continue\n","\n","        known_text, unknown_text = extract_text_from_files(problem_file_paths)\n","        label = 1 if label_data['problems'][index]['answer'] == 'Y' else 0\n","\n","        corpus[index] = {\n","            'known': known_text,\n","            'unknown': unknown_text,\n","            'label': label\n","        }\n","\n","    return corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVzAw15TL8CA"},"outputs":[],"source":["# Get data path\n","train_data_directory = get_data_directory_path('train_data')\n","# validation_data_directory = get_data_directory_path('val_data')\n","test_data_directory = get_data_directory_path('test_data')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SebVk4WpI9c2"},"outputs":[],"source":["# Train\n","train_content = read_json_file(get_json_file_path(train_data_directory, 'contents.json'))\n","train_labels = read_json_file(get_json_file_path(train_data_directory, 'truth.json'))\n","\n","# # Val\n","# validation_content = read_json_file(get_json_file_path(validation_data_directory, 'contents.json'))\n","# validation_labels = read_json_file(get_json_file_path(validation_data_directory, 'truth.json'))\n","\n","# Test\n","test_content = read_json_file(get_json_file_path(test_data_directory, 'contents.json'))\n","test_labels = read_json_file(get_json_file_path(test_data_directory, 'truth.json'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":603837,"status":"ok","timestamp":1691908170936,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"EhTF9bRPL7Ac","outputId":"5172d32d-4fe1-43be-cfaf-6acf13f4b77f"},"outputs":[],"source":["# Get train corpus\n","train_corpus = build_corpus(train_data_directory, train_content, train_labels)\n","\n","# # Get val corpus\n","# val_corpus = build_corpus(validation_data_directory, validation_content, validation_labels)\n","\n","# Get test corpus\n","test_corpus = build_corpus(test_data_directory, test_content, test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfbOPlKSoLMj"},"outputs":[],"source":["# Split the training data into training and validation\n","def split_data_into_train_and_val(data_dict, test_size=0.2, random_state=42):\n","    document_ids, labels = zip(*[(doc_id, data['label']) for doc_id, data in data_dict.items()])\n","\n","    train_ids, val_ids, train_labels, val_labels = train_test_split(document_ids, labels, test_size=test_size, random_state=random_state)\n","\n","    train_data = {doc_id: data_dict[doc_id] for doc_id in train_ids}\n","    validation_data = {doc_id: data_dict[doc_id] for doc_id in val_ids}\n","\n","    return train_data, validation_data\n","\n","train_corpus, val_corpus = split_data_into_train_and_val(data_dict=train_corpus)"]},{"cell_type":"markdown","metadata":{"id":"RkBeR4eOPEm2"},"source":["# Train Word2Vec Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wuFlKWaRJ1c"},"outputs":[],"source":["def preprocess_text(text):\n","    \"\"\"\n","    Preprocess a given text by tokenizing, removing punctuation and numbers,\n","    removing stop words, and lemmatizing.\n","\n","    Args:\n","        text (str): The text to preprocess.\n","\n","    Returns:\n","        list: The preprocessed text as a list of tokens.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        text = str(text)\n","\n","    # Tokenize the text into words\n","    tokens = word_tokenize(text.lower())\n","\n","    # Remove punctuation and numbers\n","    table = str.maketrans('', '', string.punctuation + string.digits)\n","    tokens = [word.translate(table) for word in tokens]\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if (not word in stop_words) and (word != '')]\n","\n","    # Lemmatize words\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    return tokens\n","\n","def train_word2vec_model(data, vector_size):\n","    \"\"\"\n","    Train a word2vec model using the given data.\n","\n","    Args:\n","        data (dict): The data to use for training the model.\n","        vector_size (int): The size of the word vectors in the model.\n","\n","    Returns:\n","        gensim.models.Word2Vec: The trained word2vec model.\n","    \"\"\"\n","    corpus = []\n","\n","    # Process all articles in the data\n","    for articles in tqdm(data.values(), total=len(data)):\n","        all_articles = []\n","        all_articles.extend(articles['known'])\n","        all_articles.extend(articles['unknown'])\n","\n","        for article in all_articles:\n","            for line in article:\n","                text = line.strip()\n","                tokens = preprocess_text(text)\n","                corpus.append(tokens)\n","\n","    # Train the word2vec model\n","    word2vec_model = gensim.models.Word2Vec(vector_size=vector_size, window=5, min_count=1, workers=4)\n","    word2vec_model.build_vocab(corpus)\n","    word2vec_model.train(corpus, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n","\n","    return word2vec_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVvsP3-_RS5_"},"outputs":[],"source":["# Size of word vectors in the word2vec model\n","w2v_vector_size = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16496,"status":"ok","timestamp":1691909151659,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"Ejtm_oXrRP4-","outputId":"75e52498-fd98-4398-cea4-6a93b20b62fe"},"outputs":[],"source":["# Train a word2vec model using the training corpus\n","# Bryce: modified to auto save/load models\n","WORD2VEC_SAVE = \"Word2Vec.model\"\n","\n","if os.path.isfile(WORD2VEC_SAVE):\n","\tword2vec_model = gensim.models.Word2Vec.load(WORD2VEC_SAVE)\n","else:\n","\tword2vec_model = train_word2vec_model(train_corpus, w2v_vector_size)\n","\tword2vec_model.save(WORD2VEC_SAVE)"]},{"cell_type":"markdown","metadata":{"id":"uFDDT_rmRokt"},"source":["# Vectorize Text Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDsYcGE-TyUc"},"outputs":[],"source":["def convert_text_to_vector(texts, model):\n","    \"\"\"\n","    Convert a list of texts into their corresponding word2vec vectors\n","    \"\"\"\n","    vectors = []\n","    for text in texts:\n","        words = preprocess_text(text)\n","        vector = np.sum([model.wv[word] for word in words if word in model.wv], axis=0)\n","        word_count = np.sum([word in model.wv for word in words])\n","        if word_count != 0:\n","            vector /= word_count\n","        else:\n","          vector = np.zeros(w2v_vector_size)\n","        vectors.append(vector)\n","    return vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1F6DWIL9UZIw"},"outputs":[],"source":["def count_punctuations(texts):\n","  \"\"\"\n","  Count the frequency of different punctuations in the texts\n","  \"\"\"\n","  # Define punctuations to count\n","  punctuations = set(['.', ',', ';', ':', '!', '?', '-', '(', ')', '\\\"', '\\'', '`', '/'])\n","\n","  # Initialize dictionary to count punctuations\n","  punctuations_count = {p: 0 for p in punctuations}\n","\n","  # Count punctuations in text_list\n","  for text in texts:\n","      for char in text:\n","          if char in punctuations:\n","              punctuations_count[char] += 1\n","\n","  # Return list of punctuation counts\n","  return list(punctuations_count.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_IQDJiqUX7C"},"outputs":[],"source":["def analyze_sentence_lengths(sentences):\n","  \"\"\"\n","  Analyze the lengths of sentences\n","  \"\"\"\n","  sentence_lengths = [len(sentence.split()) for sentence in sentences]\n","  average_length = np.mean(sentence_lengths)\n","  count_over_avg = np.sum([length > average_length for length in sentence_lengths])\n","  count_under_avg = np.sum([length < average_length for length in sentence_lengths])\n","  count_avg = len(sentence_lengths) - count_over_avg - count_under_avg\n","\n","  return [count_over_avg, count_under_avg, count_avg, average_length]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pevRcCjGUWyz"},"outputs":[],"source":["def analyze_words(texts):\n","    \"\"\"\n","    Analyze the words used in the texts\n","    \"\"\"\n","    words = []\n","    stop_words = set(stopwords.words('english'))\n","    lemmatizer = WordNetLemmatizer()\n","    for text in texts:\n","        tokenized = word_tokenize(text.lower())\n","        processed = [lemmatizer.lemmatize(word) for word in tokenized if word not in stop_words]\n","        words += processed\n","    word_freq = nltk.FreqDist(words)\n","    rare_count = np.sum([freq <= 2 for word, freq in word_freq.items()])\n","    long_count = np.sum([len(word) > 6 for word in words])\n","    word_lengths = [len(word) for word in words]\n","    average_length = np.mean(word_lengths)\n","    count_over_avg = np.sum([length > average_length for length in word_lengths])\n","    count_under_avg = np.sum([length < average_length for length in word_lengths])\n","    count_avg = len(word_lengths) - count_over_avg - count_under_avg\n","    ttr = len(set(words)) / len(words) if words else 0\n","\n","    return [rare_count, long_count, count_over_avg, count_under_avg, count_avg, ttr]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoTx1ZxWUUOE"},"outputs":[],"source":["def calculate_style_vector(texts):\n","  \"\"\"\n","  Calculate the style vector of the texts\n","  \"\"\"\n","  punctuation_vec = count_punctuations(texts)     # Punctuations stylistic features\n","  sentence_vec = analyze_sentence_lengths(texts)  # Sentences stylistic features\n","  word_vec = analyze_words(texts)                 # Words stylistic features\n","  word_count = np.sum([len(text.split()) for text in texts])\n","\n","  vector = np.concatenate((punctuation_vec, sentence_vec, word_vec))\n","\n","  return vector / word_count if word_count else vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SsJRlqF8gHyO"},"outputs":[],"source":["def get_vectors(texts, w2v_model):\n","  res = []\n","  for text in texts:\n","    w2v_vec = np.mean(convert_text_to_vector(text, w2v_model), axis=0)\n","    style_vec = calculate_style_vector(text)\n","    res.append(np.concatenate((w2v_vec, style_vec), axis=None))\n","    # res.append(w2v_vec)\n","\n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JEYwCl8yR0D_"},"outputs":[],"source":["def vectorize_text_data(data, w2v_model):\n","  \"\"\"\n","  Build author data from the corpus\n","  \"\"\"\n","  res = {}\n","  for key,val in tqdm(data.items(), total=len(data)):\n","    if len(val['unknown']) == 0:\n","      continue\n","    res[key] = {\n","        'known': get_vectors(val['known'], w2v_model),\n","        'unknown': get_vectors(val['unknown'], w2v_model),\n","        'label': val['label']\n","    }\n","\n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50409,"status":"ok","timestamp":1691909215431,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"bP_a-J_xX4Vf","outputId":"769b27e8-c767-4401-a617-1d1528a66916"},"outputs":[],"source":["train_data = vectorize_text_data(train_corpus, word2vec_model)\n","val_data = vectorize_text_data(val_corpus, word2vec_model)\n","test_data = vectorize_text_data(test_corpus, word2vec_model)"]},{"cell_type":"markdown","metadata":{"id":"THrYBCxhimox"},"source":["# Build Triplet Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5t503c3ip6H"},"outputs":[],"source":["# Random triplet mining\n","def build_random_triplet_sample(data):\n","  \"\"\"\n","  This function creates random triplet samples from the input data\n","  \"\"\"\n","\n","  keys_list = list(data.keys())\n","  triplet_samples = {}\n","\n","  # Initialize the lists for storing the anchor, positive, and negative samples\n","  anchors, positives, negatives = [], [], []\n","\n","  for key,val in tqdm(data.items(), total=len(data)):\n","    n = len(val['known'])\n","    for i in range(n):\n","      for j in range(i+1, n):\n","        anchors.append(val['known'][i])\n","        positives.append(val['known'][j])\n","        # Get negative sample\n","        while True:\n","          random_key = random.choices(keys_list, k=1)\n","          if random_key != key:\n","            break\n","        random_neg_sample = random.choices(data[random_key[0]]['known'], k=1)\n","        negatives.append(random_neg_sample[0])\n","\n","\n","  # Build triplet sample\n","  for i in range(len(anchors)):\n","    triplet_samples[i] = {\n","        'anchor': anchors[i],\n","        'positive': positives[i],\n","        'negative': negatives[i]\n","    }\n","\n","  return triplet_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1691909221418,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"wu8oSknIi65X","outputId":"c4302639-9af3-41eb-9433-a718a273bfda"},"outputs":[],"source":["random_triplet_samples = build_random_triplet_sample(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8eVVLqAmen1"},"outputs":[],"source":["anchor_data = np.array([data['anchor'] for data in random_triplet_samples.values()])\n","positive_data = np.array([data['positive'] for data in random_triplet_samples.values()])\n","negative_data = np.array([data['negative'] for data in random_triplet_samples.values()])\n","labels_data = np.array([0 for _ in random_triplet_samples.values()])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":592,"status":"ok","timestamp":1691909310134,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"LAMdu9OrJk35","outputId":"c1ed5b16-4a05-4a65-db22-fed3dbf79016"},"outputs":[],"source":["val_random_triplet_samples = build_random_triplet_sample(val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1Qc49FzJl2K"},"outputs":[],"source":["val_anchor_data = np.array([data['anchor'] for data in val_random_triplet_samples.values()])\n","val_positive_data = np.array([data['positive'] for data in val_random_triplet_samples.values()])\n","val_negative_data = np.array([data['negative'] for data in val_random_triplet_samples.values()])\n","val_labels_data = np.array([0 for _ in val_random_triplet_samples.values()])"]},{"cell_type":"markdown","metadata":{"id":"ryJTRnGkmlqm"},"source":["# Build SiameseNet Model"]},{"cell_type":"markdown","metadata":{"id":"mmK7NtP31jdn"},"source":["## Model Frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovKbyGoM079s"},"outputs":[],"source":["class SiameseNet(tf.keras.Model):\n","    def __init__(self, base_network, clf_network):\n","        super().__init__()\n","        self.base = base_network\n","        self.clf = clf_network\n","\n","    def call(self, inputs):\n","        anchor = inputs[0]\n","        positive = inputs[1]\n","        negative = inputs[2]\n","\n","        output_anchor = self.base(anchor)\n","        output_positive = self.base(positive)\n","        output_negative = self.base(negative)\n","\n","        # Anchor - Positive\n","        x1 = tf.concat([output_anchor, output_positive], axis=-1)\n","        x1_out = self.clf(x1)\n","\n","        # Anchor - Negative\n","        x2 = tf.concat([output_anchor, output_negative], axis=-1)\n","        x2_out = self.clf(x2)\n","\n","        return (x1_out, x2_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGlosr0W7loe"},"outputs":[],"source":["def create_dense_block(x, units, dropout_rate, l1_reg, l2_reg):\n","    x = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","    return tf.keras.layers.Dropout(dropout_rate)(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10yA51LpmofZ"},"outputs":[],"source":["# Define the base network\n","def create_base_network(embedding_dim, dropout_rate=0.4, l1_reg=0.001, l2_reg=0.001):\n","    input = tf.keras.layers.Input(shape=embedding_dim)\n","    x = tf.keras.layers.BatchNormalization()(input)\n","\n","    x = create_dense_block(x, 256, dropout_rate, l1_reg, l2_reg)\n","    x = create_dense_block(x, 128, dropout_rate, l1_reg, l2_reg)\n","    x = create_dense_block(x, 64, dropout_rate, l1_reg, l2_reg)\n","\n","    x = tf.keras.layers.Dense(64, activation='linear')(x)\n","\n","    return tf.keras.Model(inputs=input, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hW83mAPuzCDW"},"outputs":[],"source":["def create_clf_network(input_shape, dropout_rate=0.5, l1_reg=0.003, l2_reg=0.003):\n","    input = tf.keras.layers.Input(shape=(input_shape,))\n","    x = tf.keras.layers.BatchNormalization()(input)\n","\n","    x = create_dense_block(x, 128, dropout_rate, l1_reg, l2_reg)\n","    x = create_dense_block(x, 64, dropout_rate, l1_reg, l2_reg)\n","    x = create_dense_block(x, 32, dropout_rate, l1_reg, l2_reg)\n","\n","    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","    return tf.keras.Model(inputs=input, outputs=x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSgJG94HkwYj"},"outputs":[],"source":["def customer_loss(y_true, y_pred):\n","    AP = y_pred[0]\n","    AN = y_pred[1]\n","\n","    loss = 1.0 - AP + AN\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"FC2AmF8om5V4"},"source":["## Construct the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iPD1M5im982"},"outputs":[],"source":["# Define the embedding dimension\n","try:\n","\tembedding_dim = anchor_data[0].shape\n","except NameError:\n","\tembedding_dim = (323,)\n","\n","# Create base network\n","base_network = create_base_network(embedding_dim)\n","clf_network = create_clf_network(base_network.output_shape[1]*2)\n","\n","siamese_model = SiameseNet(base_network, clf_network)\n","\n","input_anchor = tf.keras.layers.Input(shape=embedding_dim)\n","input_positive = tf.keras.layers.Input(shape=embedding_dim)\n","input_negative = tf.keras.layers.Input(shape=embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8aeEYUGm__W"},"outputs":[],"source":["input_anchor = tf.keras.layers.Input(shape=embedding_dim)\n","input_positive = tf.keras.layers.Input(shape=embedding_dim)\n","input_negative = tf.keras.layers.Input(shape=embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FijK9xqnCor"},"outputs":[],"source":["# Assemble siameseNet model\n","siamese_model.compile(optimizer='adam',\n","                      loss=customer_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtusEHcYnFAZ"},"outputs":[],"source":["checkpoint_path = \"model_weights/cp.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create a callback that saves the model's weights\n","cp_save = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                             save_weights_only=True,\n","                                             verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"ssE3i-v1nJZe"},"source":["## Load SiameseNet Model Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2701,"status":"ok","timestamp":1685571625702,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"oizjc50UnGYr","outputId":"b73a8875-6d6f-494c-f02a-d56af6daf5f8"},"outputs":[],"source":["latest = tf.train.latest_checkpoint(checkpoint_dir)\n","siamese_model.load_weights(latest)"]},{"cell_type":"markdown","metadata":{"id":"Oo08aYawnUFS"},"source":["## Train SiameseNet Model"]},{"cell_type":"markdown","metadata":{"id":"lJAUUc9EnWpF"},"source":["### Train on Random Triplet Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265718,"status":"ok","timestamp":1691911932561,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"3VgFc5gnnZZa","outputId":"cd3ef38e-543f-4fdd-d07a-c50380559d0b"},"outputs":[],"source":["# Train siameseNet model\n","early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n","siamese_history = siamese_model.fit([anchor_data, positive_data, negative_data], labels_data,\n","                  epochs=1000,\n","                  validation_data=([val_anchor_data, val_positive_data, val_negative_data], val_labels_data),\n","                  callbacks=[early_stopping, cp_save])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":59661,"status":"ok","timestamp":1691911997730,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"kYqKKoldWObe","outputId":"69f34e82-7715-46a3-f28f-8d0af7c041d7"},"outputs":[],"source":["loss = siamese_history.history['loss']\n","val_loss = siamese_history.history['val_loss']\n","\n","# Draw\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.title('Loss and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zRB88fJtngeR"},"source":["### Train on Semi-Hard Triplet Samples"]},{"cell_type":"markdown","metadata":{"id":"yktUMRHfrUfQ"},"source":["#### Semi-Hard Samples Construct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7n3UK1InkZ0"},"outputs":[],"source":["# Build semi-hard triplet sample candidates\n","def build_triplet_sample_candidates(data):\n","  res = {}\n","\n","  keys = []\n","  anchors = []\n","  positives = []\n","\n","  for key,val in tqdm(data.items(), total=len(data)):\n","    n = len(val['known'])\n","    for i in range(n-1):\n","      keys.append(key)\n","      anchors.append(val['known'][i])\n","      positives.append(val['known'][i+1:])\n","\n","  for i in range(len(keys)):\n","    res[i] = {\n","        'key': keys[i],\n","        'anchor': anchors[i],\n","        'positives': positives[i]\n","    }\n","\n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1691912143570,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"k3IvyvFBpmYp","outputId":"87dac548-77ef-4c55-f6ff-056938dd5715"},"outputs":[],"source":["triplet_sample_candidates = build_triplet_sample_candidates(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58rM10jr35k4"},"outputs":[],"source":["def create_negative_vectors_dict(data):\n","    negative_vectors_dict = {}\n","    key_list = list(data.keys())\n","\n","    for key in tqdm(key_list, total=len(key_list)):\n","        negative_vectors_dict[key] = []\n","        for k,v in data.items():\n","            if k != key:\n","                for vec in v['known']:\n","                    negative_vectors_dict[key].append(vec)\n","    return negative_vectors_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1691912143571,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"hvTKNBCw5Hgp","outputId":"a733a5bc-1d07-41e4-e3b3-1bbb3b283ac6"},"outputs":[],"source":["negative_vectors_dict = create_negative_vectors_dict(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGz9M-KEqaAw"},"outputs":[],"source":["def select_random_from_list(input_list):\n","    \"\"\"\n","    Selects a random item from a list.\n","    \"\"\"\n","    return input_list[np.random.randint(0, len(input_list))]\n","\n","# def select_negative_vectors(negatives, key):\n","#     \"\"\"\n","#     Collects all negative vectors except for the one corresponding to the key.\n","#     \"\"\"\n","#     return [vec for k,v in negatives.items() if k != key for vec in v['known']]\n","\n","def get_random_triplet(sample, negatives):\n","    \"\"\"\n","    This function takes a sample and negatives, and returns a random triplet of anchor, positive, and negative.\n","    \"\"\"\n","    # Select the positive vector\n","    positive = select_random_from_list(sample['positives'])\n","\n","    # Select the negative vector\n","    negative = select_random_from_list(negatives[sample['key']])\n","\n","    return sample['anchor'], positive, negative\n","\n","def get_hard_triplet(sample, negatives, base_model, clf_model,):\n","    \"\"\"\n","    This function takes a sample, negatives, and a model, and returns a hard triplet of anchor, positive, and negative.\n","    The sample with the lowest probability is the hardest positive sample,\n","    while a high probability indicates that the model is confident in classifying it as positive.\n","    Therefore, the lowest probability implies that the model has incorrectly classified it.\n","    \"\"\"\n","    anchor_rep = base_model.predict(np.array([sample['anchor']]), verbose=0)\n","\n","    ### ------ Positive ------ ###\n","    # Compute distances between anchor and all positive vectors\n","    positive_reps = base_model.predict(np.array(sample['positives']), verbose=0)\n","    AP_reps = []\n","    for rep in positive_reps:\n","        comb = np.concatenate((anchor_rep[0], rep), axis=None)\n","        AP_reps.append(comb)\n","\n","    # Select the hardest positive (the one with the lowest probability)\n","    positive_distances = clf_model.predict(np.array(AP_reps), verbose=0)\n","    hardest_positive = sample['positives'][np.argmin(positive_distances)]\n","\n","\n","    ### ------ Negative ------ ###\n","    # Collect all negative vectors and compute distances to anchor\n","    negative_vectors = negatives[sample['key']]\n","    negative_reps = base_model.predict(np.array(negative_vectors), verbose=0)\n","    AN_reps = []\n","    for rep in negative_reps:\n","        comb = np.concatenate((anchor_rep[0], rep), axis=None)\n","        AN_reps.append(comb)\n","\n","    # Select the hardest negative (the one with the highest probability)\n","    negative_distances = clf_model.predict(np.array(AN_reps), verbose=0)\n","    hardest_negative = negative_vectors[np.argmax(negative_distances)]\n","\n","    # # positive_distances = [compute_cosine_distance(pos_rep, anchor_rep[0]) for pos_rep in positive_reps]\n","    # positive_distances = [np.sum(np.square(pos_rep - anchor_rep[0])) for pos_rep in positive_reps]\n","\n","    # # Select the hardest positive (the one with the largest distance)\n","    # hardest_positive = sample['positives'][np.argmax(positive_distances)]\n","\n","    # # Collect all negative vectors and compute distances to anchor\n","    # negative_vectors = select_negative_vectors(negatives, sample['key'])\n","    # negative_reps = model.predict(np.array(negative_vectors), verbose=0)\n","    # negative_distances = [np.sum(np.square(neg_rep - anchor_rep[0])) for neg_rep in negative_reps]\n","\n","    # # Select the hardest negative (the one with the smallest distance)\n","    # hardest_negative = negative_vectors[np.argmin(negative_distances)]\n","\n","    return sample['anchor'], hardest_positive, hardest_negative\n","\n","def get_triplet(sample, negatives, base_model, clf_model, hard_triplet_probability):\n","    \"\"\"\n","    This function decides between selecting a hard triplet or a random triplet based on the hard_triplet_probability.\n","    \"\"\"\n","    if np.random.rand() < hard_triplet_probability:\n","        # With a certain probability, choose the hardest triplet\n","        return get_hard_triplet(sample, negatives, base_model, clf_model)\n","    else:\n","        # Otherwise, choose a random triplet\n","        return get_random_triplet(sample, negatives)\n"]},{"cell_type":"markdown","metadata":{"id":"uLXLb3g8rOrA"},"source":["#### Training on Semi-Hard Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJqrVYYqrsfn"},"outputs":[],"source":["num_epochs = 100\n","patience = 10\n","previous_loss = float('inf')\n","\n","hard_triplet_probability_start=0.5\n","hard_triplet_probability_end=0.8\n","\n","early_stopping_2 = EarlyStopping(monitor='loss', patience=patience, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2207444,"status":"ok","timestamp":1691914359235,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"bWfTVmvZrxav","outputId":"82847ff4-fcdf-4767-de04-8dde24f6554b"},"outputs":[],"source":["# Initial probability of selecting a hard triplet\n","triplet_select_probability = hard_triplet_probability_start\n","\n","# Iterate over each epoch\n","for epoch in tqdm(range(num_epochs)):\n","  # Initialize empty lists for anchor, positive, negative samples and labels\n","  anchor_samples = []\n","  positive_samples = []\n","  negative_samples = []\n","  labels = []\n","\n","  # Iterate over triplet samples\n","  for _, sample in triplet_sample_candidates.items():\n","    # Get the anchor, positive, negative samples\n","    anchor, positive, negative = get_triplet(sample, negative_vectors_dict, base_network, clf_network, triplet_select_probability)\n","    # Add samples to their respective lists\n","    anchor_samples.append(anchor)\n","    positive_samples.append(positive)\n","    negative_samples.append(negative)\n","    labels.append(0)\n","\n","  # Convert lists to numpy arrays\n","  anchor_samples = np.array(anchor_samples)\n","  positive_samples = np.array(positive_samples)\n","  negative_samples = np.array(negative_samples)\n","  labels = np.array(labels)\n","\n","  # Train the model on current epoch's data\n","  siamese_model.fit([anchor_samples, positive_samples, negative_samples], labels,\n","                    epochs=50,\n","                    verbose=1,\n","                    callbacks=[early_stopping_2, cp_save])\n","\n","  # Gradually increase the probability of choosing a hard triplet\n","  triplet_select_probability += (hard_triplet_probability_end - hard_triplet_probability_start) / num_epochs\n","\n","  # Uncomment the following section for Early Stopping\n","  # Check if current epoch is a 'patience' epoch\n","  if epoch % patience == 0 and epoch != 0:\n","    current_loss = siamese_model.history.history['loss'][-1]\n","    # Check if loss is increasing or constant, if yes, then stop training\n","    if current_loss >= previous_loss:\n","      print(\"Early stopping triggered. Stopping training.\")\n","      break\n","    else:\n","      # Update previous loss with current loss\n","      previous_loss = current_loss"]},{"cell_type":"markdown","metadata":{"id":"M9k4beR9tu4o"},"source":["# Inference and Validataion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bdy5TBdrtxTV"},"outputs":[],"source":["def generate_concatenated_vectors(data, base_network):\n","  concatenated_vectors = []\n","  labels = []\n","\n","  for k, v in tqdm(data.items(), total=len(data)):\n","    # Process known vectors\n","    known_feature_vectors = base_network.predict(np.array(v['known']), verbose=0)\n","\n","    # Process unknown vectors\n","    unknown_feature_vectors = base_network.predict(np.array(v['unknown']), verbose=0)\n","\n","    # Compute the average feature vector\n","    author_representation = np.mean(known_feature_vectors, axis=0)\n","    unknown_representation = np.mean(unknown_feature_vectors, axis=0)\n","\n","    concate_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n","\n","    concatenated_vectors.append(concate_vec)\n","    labels.append(v['label'])\n","\n","  return np.array(concatenated_vectors), np.array(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89035,"status":"ok","timestamp":1691914552632,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"_AxJfKnxuRHm","outputId":"5d3c4ee4-95b2-4f38-f6a7-2ac475824ae7"},"outputs":[],"source":["# Build train siamese_embedding dataset\n","train_siamese_vec, train_siamese_labels = generate_concatenated_vectors(train_data, base_network)\n","\n","# Build val siamese_embedding dataset\n","val_siamese_vec, val_siamese_labels = generate_concatenated_vectors(val_data, base_network)\n","\n","# Build test siamese_embedding dataset\n","test_siamese_vec, test_siamese_labels = generate_concatenated_vectors(test_data, base_network)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYLuWkM_QbYR"},"outputs":[],"source":["clf_network.compile(optimizer='adam',\n","                    loss='binary_crossentropy',\n","                    metrics=['accuracy', tf.keras.metrics.AUC()])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42284,"status":"ok","timestamp":1691914594914,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"53b-IY7I2xh-","outputId":"d39029da-8d01-4fc2-c7b8-3168fa2314dd"},"outputs":[],"source":["clf_early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n","clf_history = clf_network.fit(train_siamese_vec, train_siamese_labels,\n","                              epochs=1000,\n","                              verbose=1,\n","                              validation_data = (val_siamese_vec, val_siamese_labels),\n","                              callbacks=[clf_early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38904,"status":"ok","timestamp":1691914638758,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"sehqxBD07HCR","outputId":"99ab3db6-a86f-4722-b6ff-17d93d779b4d"},"outputs":[],"source":["res = clf_network.evaluate(test_siamese_vec, test_siamese_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":442,"status":"ok","timestamp":1686567476827,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"40B63860CvLq","outputId":"06ba1675-7584-4a57-f82a-28c43832f68e"},"outputs":[],"source":["# res = clf_network.evaluate(test_siamese_vec, test_siamese_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bryce - code to save/load the entire model\n","SIAMESENET_MODEL_SAVE = \"basenet.keras\"\n","\n","if os.path.isfile(SIAMESENET_MODEL_SAVE):\n","\tsiamese_model = tf.keras.models.load_model(SIAMESENET_MODEL_SAVE)\n","\tbase_network, clf_network = siamese_model.base, siamese_model.clf\n","else:\n","\ttf.keras.Model.save(siamese_model, SIAMESENET_MODEL_SAVE)"]},{"cell_type":"markdown","metadata":{"id":"C89dzA6bxVsf"},"source":["# Calculate Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRskuFpyxYFx"},"outputs":[],"source":["def calculate_score(y_predict, y_true):\n","    n = len(y_predict)\n","    n_correct = 0\n","    n_unknown = 0\n","\n","    for i in range(n):\n","        if y_predict[i] > 0.5:\n","            prediction = 1\n","        elif y_predict[i] < 0.5:\n","            prediction = 0\n","        else:\n","            n_unknown += 1\n","            continue\n","\n","        if prediction == y_true[i]:\n","            n_correct += 1\n","\n","    c_1 = (n_correct + (n_unknown * n_correct / n)) / n\n","    auc = tf.keras.metrics.AUC()(y_true, y_predict)\n","    score = auc.numpy() * c_1\n","\n","    return c_1, auc.numpy(), score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1691914656447,"user":{"displayName":"玉张","userId":"00689396698697732491"},"user_tz":-600},"id":"tdsN95EnxZhj","outputId":"bdb8bd43-648c-406e-aee8-d100f0976405"},"outputs":[],"source":["nn_pred = clf_network.predict(test_siamese_vec)\n","c_1, auc, score = calculate_score(nn_pred, test_siamese_labels)\n","\n","print(\"C@1:\", round(c_1, 3))\n","print(\"AUC:\", round(auc, 3))\n","print(\"Final Score:\", round(score, 3))"]},{"cell_type":"markdown","metadata":{"id":"rjYlgh0rVNXc"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEQOtzt8VPSS"},"outputs":[],"source":["# Different Window Size\n","# ------ 5 ------\n","# C@1: 0.695\n","# AUC: 0.77\n","# Final Score: 0.535\n","\n","# ------ 4 ------\n","\n","\n","\n","# ------ 3 ------\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Use on Arbitrary Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def vectorize_set(known,unknown,w2v_model):\n","    \"\"\"Converts a single set of texts, instead of a corpus, to vectors\"\"\"\n","    vectors = {\n","        'known': get_vectors(known, w2v_model),\n","        'unknown': get_vectors(unknown, w2v_model)\n","        # label not included since it's not relevant or known\n","\t}\n","    return vectors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def concatenate_vector_set(vectors, base_network):\n","    known_feature_vectors = base_network.predict(np.array(vectors['known']), verbose=0)\n","    unknown_feature_vectors = base_network.predict(np.array(vectors['unknown']), verbose=0)\n","    \n","    author_representation = np.mean(known_feature_vectors, axis=0)\n","    unknown_representation = np.mean(unknown_feature_vectors, axis=0)\n","    \n","    concate_vec = np.concatenate((author_representation, unknown_representation), axis=None)\n","    return concate_vec\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_once(path, w2v_model, base_network):\n","    files = []\n","    for f in os.listdir(path):\n","        files.append(f\"{path}/{f}\")\n","\n","    known_data, unknown_data = extract_text_from_files(files)\n","    \n","    vectors = vectorize_set(known_data, unknown_data, w2v_model)\n","    concats = concatenate_vector_set(vectors, base_network)\n","\n","    prediction = clf_network.predict(np.array([concats]))\n","    return prediction[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(predict_once(\"tests\", word2vec_model, base_network))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Testing the consistency of the individual use functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOL0FSGYKjNlG212nMRFyv8","collapsed_sections":["Xt4R6YMSF7sG","Mepv72EIF_kq","RkBeR4eOPEm2","uFDDT_rmRokt","THrYBCxhimox","ryJTRnGkmlqm","ssE3i-v1nJZe","lJAUUc9EnWpF","M9k4beR9tu4o","C89dzA6bxVsf","rjYlgh0rVNXc"],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
